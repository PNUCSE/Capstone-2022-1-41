{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-21 14:53:45.322744: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import keras.callbacks\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import Sequential\n",
    "import sklearn\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import regularizers"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-21 14:53:46.653453: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-21 14:53:46.671823: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-09-21 14:53:46.671835: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-09-21 14:53:46.672242: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135/155 [=========================>....] - ETA: 0s - loss: 2.1911 - accuracy: 0.6690 - recall_m: 0.4365 - precision_m: 0.8358 - f1_m: 0.5601\n",
      "Epoch 1: val_loss improved from inf to 1.36789, saving model to trained_models/fall_model_five_class.h5\n",
      "155/155 [==============================] - 1s 2ms/step - loss: 2.0882 - accuracy: 0.6929 - recall_m: 0.4743 - precision_m: 0.8436 - f1_m: 0.5916 - val_loss: 1.3679 - val_accuracy: 0.8012 - val_recall_m: 0.7247 - val_precision_m: 0.8357 - val_f1_m: 0.7758\n",
      "Epoch 2/300\n",
      "124/155 [=======================>......] - ETA: 0s - loss: 1.0389 - accuracy: 0.8850 - recall_m: 0.8182 - precision_m: 0.9167 - f1_m: 0.8642\n",
      "Epoch 2: val_loss improved from 1.36789 to 0.86148, saving model to trained_models/fall_model_five_class.h5\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.9947 - accuracy: 0.8866 - recall_m: 0.8257 - precision_m: 0.9163 - f1_m: 0.8681 - val_loss: 0.8615 - val_accuracy: 0.8413 - val_recall_m: 0.7879 - val_precision_m: 0.8620 - val_f1_m: 0.8230\n",
      "Epoch 3/300\n",
      "122/155 [======================>.......] - ETA: 0s - loss: 0.6734 - accuracy: 0.9102 - recall_m: 0.8849 - precision_m: 0.9306 - f1_m: 0.9069\n",
      "Epoch 3: val_loss improved from 0.86148 to 0.64458, saving model to trained_models/fall_model_five_class.h5\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.6520 - accuracy: 0.9129 - recall_m: 0.8874 - precision_m: 0.9321 - f1_m: 0.9090 - val_loss: 0.6446 - val_accuracy: 0.8894 - val_recall_m: 0.8593 - val_precision_m: 0.9081 - val_f1_m: 0.8828\n",
      "Epoch 4/300\n",
      "135/155 [=========================>....] - ETA: 0s - loss: 0.5171 - accuracy: 0.9253 - recall_m: 0.9080 - precision_m: 0.9414 - f1_m: 0.9243\n",
      "Epoch 4: val_loss improved from 0.64458 to 0.56515, saving model to trained_models/fall_model_five_class.h5\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.5163 - accuracy: 0.9240 - recall_m: 0.9068 - precision_m: 0.9397 - f1_m: 0.9228 - val_loss: 0.5651 - val_accuracy: 0.9009 - val_recall_m: 0.8786 - val_precision_m: 0.9132 - val_f1_m: 0.8955\n",
      "Epoch 5/300\n",
      "151/155 [============================>.] - ETA: 0s - loss: 0.4442 - accuracy: 0.9396 - recall_m: 0.9246 - precision_m: 0.9498 - f1_m: 0.9369\n",
      "Epoch 5: val_loss improved from 0.56515 to 0.50306, saving model to trained_models/fall_model_five_class.h5\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.4442 - accuracy: 0.9395 - recall_m: 0.9245 - precision_m: 0.9497 - f1_m: 0.9369 - val_loss: 0.5031 - val_accuracy: 0.9070 - val_recall_m: 0.8935 - val_precision_m: 0.9185 - val_f1_m: 0.9057\n",
      "Epoch 6/300\n",
      "140/155 [==========================>...] - ETA: 0s - loss: 0.4041 - accuracy: 0.9449 - recall_m: 0.9321 - precision_m: 0.9558 - f1_m: 0.9438\n",
      "Epoch 6: val_loss improved from 0.50306 to 0.47338, saving model to trained_models/fall_model_five_class.h5\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.4044 - accuracy: 0.9439 - recall_m: 0.9304 - precision_m: 0.9548 - f1_m: 0.9423 - val_loss: 0.4734 - val_accuracy: 0.9125 - val_recall_m: 0.9057 - val_precision_m: 0.9195 - val_f1_m: 0.9125\n",
      "Epoch 7/300\n",
      "126/155 [=======================>......] - ETA: 0s - loss: 0.3790 - accuracy: 0.9446 - recall_m: 0.9360 - precision_m: 0.9546 - f1_m: 0.9451\n",
      "Epoch 7: val_loss improved from 0.47338 to 0.45630, saving model to trained_models/fall_model_five_class.h5\n",
      "155/155 [==============================] - 0s 983us/step - loss: 0.3787 - accuracy: 0.9456 - recall_m: 0.9367 - precision_m: 0.9555 - f1_m: 0.9460 - val_loss: 0.4563 - val_accuracy: 0.9112 - val_recall_m: 0.9016 - val_precision_m: 0.9200 - val_f1_m: 0.9107\n",
      "Epoch 8/300\n",
      "141/155 [==========================>...] - ETA: 0s - loss: 0.3606 - accuracy: 0.9496 - recall_m: 0.9412 - precision_m: 0.9577 - f1_m: 0.9493\n",
      "Epoch 8: val_loss improved from 0.45630 to 0.43007, saving model to trained_models/fall_model_five_class.h5\n",
      "155/155 [==============================] - 0s 964us/step - loss: 0.3618 - accuracy: 0.9495 - recall_m: 0.9413 - precision_m: 0.9578 - f1_m: 0.9494 - val_loss: 0.4301 - val_accuracy: 0.9146 - val_recall_m: 0.9118 - val_precision_m: 0.9232 - val_f1_m: 0.9174\n",
      "Epoch 9/300\n",
      " 84/155 [===============>..............] - ETA: 0s - loss: 0.3578 - accuracy: 0.9492 - recall_m: 0.9408 - precision_m: 0.9580 - f1_m: 0.9493\n",
      "Epoch 9: val_loss did not improve from 0.43007\n",
      "155/155 [==============================] - 0s 810us/step - loss: 0.3522 - accuracy: 0.9491 - recall_m: 0.9406 - precision_m: 0.9586 - f1_m: 0.9495 - val_loss: 0.4361 - val_accuracy: 0.9158 - val_recall_m: 0.9093 - val_precision_m: 0.9220 - val_f1_m: 0.9155\n",
      "Epoch 10/300\n",
      "151/155 [============================>.] - ETA: 0s - loss: 0.3442 - accuracy: 0.9480 - recall_m: 0.9415 - precision_m: 0.9551 - f1_m: 0.9482\n",
      "Epoch 10: val_loss improved from 0.43007 to 0.42904, saving model to trained_models/fall_model_five_class.h5\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.3437 - accuracy: 0.9483 - recall_m: 0.9414 - precision_m: 0.9558 - f1_m: 0.9485 - val_loss: 0.4290 - val_accuracy: 0.9149 - val_recall_m: 0.9124 - val_precision_m: 0.9201 - val_f1_m: 0.9162\n",
      "Epoch 11/300\n",
      "128/155 [=======================>......] - ETA: 0s - loss: 0.3317 - accuracy: 0.9563 - recall_m: 0.9485 - precision_m: 0.9624 - f1_m: 0.9553\n",
      "Epoch 11: val_loss improved from 0.42904 to 0.41970, saving model to trained_models/fall_model_five_class.h5\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.3352 - accuracy: 0.9540 - recall_m: 0.9458 - precision_m: 0.9601 - f1_m: 0.9528 - val_loss: 0.4197 - val_accuracy: 0.9185 - val_recall_m: 0.9151 - val_precision_m: 0.9259 - val_f1_m: 0.9204\n",
      "Epoch 12/300\n",
      "145/155 [===========================>..] - ETA: 0s - loss: 0.3335 - accuracy: 0.9511 - recall_m: 0.9441 - precision_m: 0.9598 - f1_m: 0.9518\n",
      "Epoch 12: val_loss did not improve from 0.41970\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.3320 - accuracy: 0.9517 - recall_m: 0.9447 - precision_m: 0.9597 - f1_m: 0.9521 - val_loss: 0.4226 - val_accuracy: 0.9185 - val_recall_m: 0.9154 - val_precision_m: 0.9242 - val_f1_m: 0.9198\n",
      "Epoch 13/300\n",
      "148/155 [===========================>..] - ETA: 0s - loss: 0.3220 - accuracy: 0.9519 - recall_m: 0.9453 - precision_m: 0.9585 - f1_m: 0.9518\n",
      "Epoch 13: val_loss improved from 0.41970 to 0.40388, saving model to trained_models/fall_model_five_class.h5\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.3223 - accuracy: 0.9516 - recall_m: 0.9450 - precision_m: 0.9578 - f1_m: 0.9513 - val_loss: 0.4039 - val_accuracy: 0.9191 - val_recall_m: 0.9160 - val_precision_m: 0.9254 - val_f1_m: 0.9206\n",
      "Epoch 14/300\n",
      "122/155 [======================>.......] - ETA: 0s - loss: 0.3203 - accuracy: 0.9539 - recall_m: 0.9477 - precision_m: 0.9607 - f1_m: 0.9541\n",
      "Epoch 14: val_loss improved from 0.40388 to 0.39588, saving model to trained_models/fall_model_five_class.h5\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.3201 - accuracy: 0.9538 - recall_m: 0.9469 - precision_m: 0.9599 - f1_m: 0.9533 - val_loss: 0.3959 - val_accuracy: 0.9213 - val_recall_m: 0.9201 - val_precision_m: 0.9270 - val_f1_m: 0.9235\n",
      "Epoch 15/300\n",
      "134/155 [========================>.....] - ETA: 0s - loss: 0.3072 - accuracy: 0.9563 - recall_m: 0.9503 - precision_m: 0.9623 - f1_m: 0.9563\n",
      "Epoch 15: val_loss improved from 0.39588 to 0.38064, saving model to trained_models/fall_model_five_class.h5\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.3075 - accuracy: 0.9563 - recall_m: 0.9504 - precision_m: 0.9622 - f1_m: 0.9562 - val_loss: 0.3806 - val_accuracy: 0.9243 - val_recall_m: 0.9202 - val_precision_m: 0.9290 - val_f1_m: 0.9246\n",
      "Epoch 16/300\n",
      "142/155 [==========================>...] - ETA: 0s - loss: 0.3070 - accuracy: 0.9557 - recall_m: 0.9494 - precision_m: 0.9614 - f1_m: 0.9553\n",
      "Epoch 16: val_loss did not improve from 0.38064\n",
      "155/155 [==============================] - 0s 878us/step - loss: 0.3049 - accuracy: 0.9564 - recall_m: 0.9501 - precision_m: 0.9624 - f1_m: 0.9562 - val_loss: 0.4086 - val_accuracy: 0.9198 - val_recall_m: 0.9159 - val_precision_m: 0.9247 - val_f1_m: 0.9202\n",
      "Epoch 17/300\n",
      "141/155 [==========================>...] - ETA: 0s - loss: 0.3017 - accuracy: 0.9568 - recall_m: 0.9504 - precision_m: 0.9625 - f1_m: 0.9564\n",
      "Epoch 17: val_loss did not improve from 0.38064\n",
      "155/155 [==============================] - 0s 931us/step - loss: 0.3020 - accuracy: 0.9570 - recall_m: 0.9504 - precision_m: 0.9627 - f1_m: 0.9565 - val_loss: 0.3978 - val_accuracy: 0.9179 - val_recall_m: 0.9159 - val_precision_m: 0.9253 - val_f1_m: 0.9205\n",
      "Epoch 18/300\n",
      "121/155 [======================>.......] - ETA: 0s - loss: 0.3032 - accuracy: 0.9583 - recall_m: 0.9518 - precision_m: 0.9637 - f1_m: 0.9577\n",
      "Epoch 18: val_loss did not improve from 0.38064\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.3011 - accuracy: 0.9579 - recall_m: 0.9511 - precision_m: 0.9627 - f1_m: 0.9568 - val_loss: 0.3819 - val_accuracy: 0.9249 - val_recall_m: 0.9213 - val_precision_m: 0.9283 - val_f1_m: 0.9247\n",
      "Epoch 19/300\n",
      "132/155 [========================>.....] - ETA: 0s - loss: 0.2994 - accuracy: 0.9567 - recall_m: 0.9503 - precision_m: 0.9628 - f1_m: 0.9564\n",
      "Epoch 19: val_loss did not improve from 0.38064\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.2965 - accuracy: 0.9572 - recall_m: 0.9509 - precision_m: 0.9635 - f1_m: 0.9571 - val_loss: 0.4051 - val_accuracy: 0.9146 - val_recall_m: 0.9121 - val_precision_m: 0.9210 - val_f1_m: 0.9165\n",
      "Epoch 20/300\n",
      "144/155 [==========================>...] - ETA: 0s - loss: 0.2903 - accuracy: 0.9572 - recall_m: 0.9508 - precision_m: 0.9629 - f1_m: 0.9568\n",
      "Epoch 20: val_loss did not improve from 0.38064\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.2900 - accuracy: 0.9579 - recall_m: 0.9517 - precision_m: 0.9634 - f1_m: 0.9575 - val_loss: 0.4022 - val_accuracy: 0.9191 - val_recall_m: 0.9172 - val_precision_m: 0.9241 - val_f1_m: 0.9206\n",
      "Epoch 21/300\n",
      "135/155 [=========================>....] - ETA: 0s - loss: 0.2956 - accuracy: 0.9569 - recall_m: 0.9509 - precision_m: 0.9631 - f1_m: 0.9569\n",
      "Epoch 21: val_loss did not improve from 0.38064\n",
      "155/155 [==============================] - 0s 978us/step - loss: 0.2942 - accuracy: 0.9573 - recall_m: 0.9517 - precision_m: 0.9634 - f1_m: 0.9575 - val_loss: 0.3834 - val_accuracy: 0.9261 - val_recall_m: 0.9226 - val_precision_m: 0.9324 - val_f1_m: 0.9275\n",
      "Epoch 22/300\n",
      "127/155 [=======================>......] - ETA: 0s - loss: 0.2824 - accuracy: 0.9609 - recall_m: 0.9541 - precision_m: 0.9656 - f1_m: 0.9598\n",
      "Epoch 22: val_loss did not improve from 0.38064\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.2839 - accuracy: 0.9594 - recall_m: 0.9531 - precision_m: 0.9645 - f1_m: 0.9587 - val_loss: 0.4138 - val_accuracy: 0.9188 - val_recall_m: 0.9136 - val_precision_m: 0.9262 - val_f1_m: 0.9198\n",
      "Epoch 23/300\n",
      "150/155 [============================>.] - ETA: 0s - loss: 0.2856 - accuracy: 0.9578 - recall_m: 0.9521 - precision_m: 0.9636 - f1_m: 0.9578\n",
      "Epoch 23: val_loss did not improve from 0.38064\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.2858 - accuracy: 0.9583 - recall_m: 0.9527 - precision_m: 0.9642 - f1_m: 0.9584 - val_loss: 0.3986 - val_accuracy: 0.9179 - val_recall_m: 0.9151 - val_precision_m: 0.9246 - val_f1_m: 0.9198\n",
      "Epoch 24/300\n",
      "150/155 [============================>.] - ETA: 0s - loss: 0.2776 - accuracy: 0.9599 - recall_m: 0.9551 - precision_m: 0.9655 - f1_m: 0.9602\n",
      "Epoch 24: val_loss did not improve from 0.38064\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.2774 - accuracy: 0.9600 - recall_m: 0.9553 - precision_m: 0.9658 - f1_m: 0.9605 - val_loss: 0.3855 - val_accuracy: 0.9195 - val_recall_m: 0.9130 - val_precision_m: 0.9240 - val_f1_m: 0.9184\n",
      "Epoch 25/300\n",
      "152/155 [============================>.] - ETA: 0s - loss: 0.2793 - accuracy: 0.9621 - recall_m: 0.9568 - precision_m: 0.9671 - f1_m: 0.9619\n",
      "Epoch 25: val_loss did not improve from 0.38064\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.2800 - accuracy: 0.9618 - recall_m: 0.9566 - precision_m: 0.9670 - f1_m: 0.9617 - val_loss: 0.4177 - val_accuracy: 0.9182 - val_recall_m: 0.9136 - val_precision_m: 0.9244 - val_f1_m: 0.9189\n",
      "Epoch 26/300\n",
      "138/155 [=========================>....] - ETA: 0s - loss: 0.2765 - accuracy: 0.9579 - recall_m: 0.9530 - precision_m: 0.9639 - f1_m: 0.9584\n",
      "Epoch 26: val_loss improved from 0.38064 to 0.37693, saving model to trained_models/fall_model_five_class.h5\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.2762 - accuracy: 0.9580 - recall_m: 0.9524 - precision_m: 0.9635 - f1_m: 0.9578 - val_loss: 0.3769 - val_accuracy: 0.9201 - val_recall_m: 0.9147 - val_precision_m: 0.9260 - val_f1_m: 0.9203\n",
      "Epoch 27/300\n",
      "121/155 [======================>.......] - ETA: 0s - loss: 0.2696 - accuracy: 0.9633 - recall_m: 0.9585 - precision_m: 0.9695 - f1_m: 0.9639\n",
      "Epoch 27: val_loss improved from 0.37693 to 0.36104, saving model to trained_models/fall_model_five_class.h5\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.2690 - accuracy: 0.9625 - recall_m: 0.9583 - precision_m: 0.9686 - f1_m: 0.9634 - val_loss: 0.3610 - val_accuracy: 0.9277 - val_recall_m: 0.9249 - val_precision_m: 0.9321 - val_f1_m: 0.9284\n",
      "Epoch 28/300\n",
      "147/155 [===========================>..] - ETA: 0s - loss: 0.2674 - accuracy: 0.9624 - recall_m: 0.9571 - precision_m: 0.9679 - f1_m: 0.9624\n",
      "Epoch 28: val_loss did not improve from 0.36104\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.2656 - accuracy: 0.9635 - recall_m: 0.9584 - precision_m: 0.9690 - f1_m: 0.9636 - val_loss: 0.3778 - val_accuracy: 0.9204 - val_recall_m: 0.9165 - val_precision_m: 0.9245 - val_f1_m: 0.9204\n",
      "Epoch 29/300\n",
      "128/155 [=======================>......] - ETA: 0s - loss: 0.2674 - accuracy: 0.9611 - recall_m: 0.9558 - precision_m: 0.9667 - f1_m: 0.9612\n",
      "Epoch 29: val_loss improved from 0.36104 to 0.35241, saving model to trained_models/fall_model_five_class.h5\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.2664 - accuracy: 0.9611 - recall_m: 0.9555 - precision_m: 0.9664 - f1_m: 0.9609 - val_loss: 0.3524 - val_accuracy: 0.9274 - val_recall_m: 0.9256 - val_precision_m: 0.9343 - val_f1_m: 0.9299\n",
      "Epoch 30/300\n",
      "150/155 [============================>.] - ETA: 0s - loss: 0.2732 - accuracy: 0.9595 - recall_m: 0.9543 - precision_m: 0.9655 - f1_m: 0.9598\n",
      "Epoch 30: val_loss did not improve from 0.35241\n",
      "155/155 [==============================] - 0s 862us/step - loss: 0.2727 - accuracy: 0.9594 - recall_m: 0.9545 - precision_m: 0.9656 - f1_m: 0.9600 - val_loss: 0.3741 - val_accuracy: 0.9243 - val_recall_m: 0.9199 - val_precision_m: 0.9273 - val_f1_m: 0.9236\n",
      "Epoch 31/300\n",
      "142/155 [==========================>...] - ETA: 0s - loss: 0.2639 - accuracy: 0.9623 - recall_m: 0.9569 - precision_m: 0.9672 - f1_m: 0.9620\n",
      "Epoch 31: val_loss did not improve from 0.35241\n",
      "155/155 [==============================] - 0s 950us/step - loss: 0.2645 - accuracy: 0.9620 - recall_m: 0.9570 - precision_m: 0.9670 - f1_m: 0.9619 - val_loss: 0.3823 - val_accuracy: 0.9225 - val_recall_m: 0.9189 - val_precision_m: 0.9274 - val_f1_m: 0.9231\n",
      "Epoch 32/300\n",
      "104/155 [===================>..........] - ETA: 0s - loss: 0.2656 - accuracy: 0.9605 - recall_m: 0.9564 - precision_m: 0.9665 - f1_m: 0.9614\n",
      "Epoch 32: val_loss did not improve from 0.35241\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.2594 - accuracy: 0.9623 - recall_m: 0.9584 - precision_m: 0.9680 - f1_m: 0.9631 - val_loss: 0.3753 - val_accuracy: 0.9222 - val_recall_m: 0.9201 - val_precision_m: 0.9255 - val_f1_m: 0.9228\n",
      "Epoch 33/300\n",
      "137/155 [=========================>....] - ETA: 0s - loss: 0.2566 - accuracy: 0.9619 - recall_m: 0.9581 - precision_m: 0.9675 - f1_m: 0.9627\n",
      "Epoch 33: val_loss did not improve from 0.35241\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.2580 - accuracy: 0.9619 - recall_m: 0.9583 - precision_m: 0.9675 - f1_m: 0.9628 - val_loss: 0.3785 - val_accuracy: 0.9243 - val_recall_m: 0.9225 - val_precision_m: 0.9287 - val_f1_m: 0.9256\n",
      "Epoch 34/300\n",
      "123/155 [======================>.......] - ETA: 0s - loss: 0.2568 - accuracy: 0.9643 - recall_m: 0.9590 - precision_m: 0.9693 - f1_m: 0.9640\n",
      "Epoch 34: val_loss did not improve from 0.35241\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.2542 - accuracy: 0.9648 - recall_m: 0.9594 - precision_m: 0.9687 - f1_m: 0.9640 - val_loss: 0.3702 - val_accuracy: 0.9249 - val_recall_m: 0.9192 - val_precision_m: 0.9300 - val_f1_m: 0.9245\n",
      "Epoch 35/300\n",
      "143/155 [==========================>...] - ETA: 0s - loss: 0.2574 - accuracy: 0.9603 - recall_m: 0.9575 - precision_m: 0.9658 - f1_m: 0.9616\n",
      "Epoch 35: val_loss did not improve from 0.35241\n",
      "155/155 [==============================] - 0s 902us/step - loss: 0.2568 - accuracy: 0.9595 - recall_m: 0.9565 - precision_m: 0.9648 - f1_m: 0.9606 - val_loss: 0.3559 - val_accuracy: 0.9243 - val_recall_m: 0.9220 - val_precision_m: 0.9287 - val_f1_m: 0.9253\n",
      "Epoch 36/300\n",
      "137/155 [=========================>....] - ETA: 0s - loss: 0.2533 - accuracy: 0.9618 - recall_m: 0.9581 - precision_m: 0.9665 - f1_m: 0.9623\n",
      "Epoch 36: val_loss improved from 0.35241 to 0.34244, saving model to trained_models/fall_model_five_class.h5\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.2519 - accuracy: 0.9625 - recall_m: 0.9587 - precision_m: 0.9668 - f1_m: 0.9627 - val_loss: 0.3424 - val_accuracy: 0.9277 - val_recall_m: 0.9276 - val_precision_m: 0.9347 - val_f1_m: 0.9311\n",
      "Epoch 37/300\n",
      "141/155 [==========================>...] - ETA: 0s - loss: 0.2541 - accuracy: 0.9623 - recall_m: 0.9591 - precision_m: 0.9666 - f1_m: 0.9628\n",
      "Epoch 37: val_loss did not improve from 0.34244\n",
      "155/155 [==============================] - 0s 883us/step - loss: 0.2534 - accuracy: 0.9625 - recall_m: 0.9596 - precision_m: 0.9668 - f1_m: 0.9631 - val_loss: 0.3611 - val_accuracy: 0.9261 - val_recall_m: 0.9234 - val_precision_m: 0.9303 - val_f1_m: 0.9268\n",
      "Epoch 38/300\n",
      "124/155 [=======================>......] - ETA: 0s - loss: 0.2495 - accuracy: 0.9630 - recall_m: 0.9590 - precision_m: 0.9683 - f1_m: 0.9636\n",
      "Epoch 38: val_loss did not improve from 0.34244\n",
      "155/155 [==============================] - 0s 993us/step - loss: 0.2478 - accuracy: 0.9647 - recall_m: 0.9604 - precision_m: 0.9693 - f1_m: 0.9648 - val_loss: 0.3628 - val_accuracy: 0.9201 - val_recall_m: 0.9177 - val_precision_m: 0.9260 - val_f1_m: 0.9218\n",
      "Epoch 39/300\n",
      "118/155 [=====================>........] - ETA: 0s - loss: 0.2486 - accuracy: 0.9629 - recall_m: 0.9587 - precision_m: 0.9686 - f1_m: 0.9636\n",
      "Epoch 39: val_loss did not improve from 0.34244\n",
      "155/155 [==============================] - 0s 999us/step - loss: 0.2491 - accuracy: 0.9630 - recall_m: 0.9589 - precision_m: 0.9678 - f1_m: 0.9633 - val_loss: 0.3630 - val_accuracy: 0.9267 - val_recall_m: 0.9244 - val_precision_m: 0.9318 - val_f1_m: 0.9281\n",
      "Epoch 40/300\n",
      "127/155 [=======================>......] - ETA: 0s - loss: 0.2445 - accuracy: 0.9653 - recall_m: 0.9612 - precision_m: 0.9704 - f1_m: 0.9658\n",
      "Epoch 40: val_loss did not improve from 0.34244\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.2457 - accuracy: 0.9647 - recall_m: 0.9600 - precision_m: 0.9686 - f1_m: 0.9642 - val_loss: 0.3686 - val_accuracy: 0.9210 - val_recall_m: 0.9186 - val_precision_m: 0.9265 - val_f1_m: 0.9225\n",
      "Epoch 41/300\n",
      "112/155 [====================>.........] - ETA: 0s - loss: 0.2555 - accuracy: 0.9605 - recall_m: 0.9568 - precision_m: 0.9658 - f1_m: 0.9612\n",
      "Epoch 41: val_loss did not improve from 0.34244\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.2554 - accuracy: 0.9615 - recall_m: 0.9583 - precision_m: 0.9666 - f1_m: 0.9624 - val_loss: 0.3773 - val_accuracy: 0.9191 - val_recall_m: 0.9172 - val_precision_m: 0.9254 - val_f1_m: 0.9213\n",
      "Epoch 42/300\n",
      "134/155 [========================>.....] - ETA: 0s - loss: 0.2433 - accuracy: 0.9660 - recall_m: 0.9620 - precision_m: 0.9711 - f1_m: 0.9665\n",
      "Epoch 42: val_loss did not improve from 0.34244\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.2428 - accuracy: 0.9666 - recall_m: 0.9631 - precision_m: 0.9715 - f1_m: 0.9673 - val_loss: 0.3840 - val_accuracy: 0.9167 - val_recall_m: 0.9141 - val_precision_m: 0.9230 - val_f1_m: 0.9185\n",
      "Epoch 43/300\n",
      "130/155 [========================>.....] - ETA: 0s - loss: 0.2462 - accuracy: 0.9619 - recall_m: 0.9577 - precision_m: 0.9690 - f1_m: 0.9633\n",
      "Epoch 43: val_loss did not improve from 0.34244\n",
      "155/155 [==============================] - 0s 943us/step - loss: 0.2484 - accuracy: 0.9612 - recall_m: 0.9568 - precision_m: 0.9675 - f1_m: 0.9621 - val_loss: 0.3699 - val_accuracy: 0.9213 - val_recall_m: 0.9183 - val_precision_m: 0.9279 - val_f1_m: 0.9231\n",
      "Epoch 44/300\n",
      "125/155 [=======================>......] - ETA: 0s - loss: 0.2445 - accuracy: 0.9630 - recall_m: 0.9600 - precision_m: 0.9685 - f1_m: 0.9642\n",
      "Epoch 44: val_loss did not improve from 0.34244\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.2413 - accuracy: 0.9636 - recall_m: 0.9601 - precision_m: 0.9692 - f1_m: 0.9646 - val_loss: 0.3689 - val_accuracy: 0.9222 - val_recall_m: 0.9201 - val_precision_m: 0.9259 - val_f1_m: 0.9229\n",
      "Epoch 45/300\n",
      " 91/155 [================>.............] - ETA: 0s - loss: 0.2421 - accuracy: 0.9669 - recall_m: 0.9634 - precision_m: 0.9716 - f1_m: 0.9675\n",
      "Epoch 45: val_loss did not improve from 0.34244\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.2430 - accuracy: 0.9649 - recall_m: 0.9608 - precision_m: 0.9701 - f1_m: 0.9654 - val_loss: 0.3588 - val_accuracy: 0.9222 - val_recall_m: 0.9189 - val_precision_m: 0.9258 - val_f1_m: 0.9223\n",
      "Epoch 46/300\n",
      "146/155 [===========================>..] - ETA: 0s - loss: 0.2412 - accuracy: 0.9635 - recall_m: 0.9599 - precision_m: 0.9678 - f1_m: 0.9638\n",
      "Epoch 46: val_loss did not improve from 0.34244\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.2394 - accuracy: 0.9639 - recall_m: 0.9606 - precision_m: 0.9684 - f1_m: 0.9644 - val_loss: 0.3903 - val_accuracy: 0.9198 - val_recall_m: 0.9181 - val_precision_m: 0.9267 - val_f1_m: 0.9224\n",
      "Epoch 47/300\n",
      "148/155 [===========================>..] - ETA: 0s - loss: 0.2319 - accuracy: 0.9678 - recall_m: 0.9638 - precision_m: 0.9724 - f1_m: 0.9681\n",
      "Epoch 47: val_loss did not improve from 0.34244\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.2333 - accuracy: 0.9674 - recall_m: 0.9636 - precision_m: 0.9720 - f1_m: 0.9677 - val_loss: 0.3555 - val_accuracy: 0.9280 - val_recall_m: 0.9247 - val_precision_m: 0.9337 - val_f1_m: 0.9292\n",
      "Epoch 48/300\n",
      "126/155 [=======================>......] - ETA: 0s - loss: 0.2407 - accuracy: 0.9674 - recall_m: 0.9633 - precision_m: 0.9710 - f1_m: 0.9671\n",
      "Epoch 48: val_loss did not improve from 0.34244\n",
      "155/155 [==============================] - 0s 964us/step - loss: 0.2401 - accuracy: 0.9661 - recall_m: 0.9624 - precision_m: 0.9700 - f1_m: 0.9662 - val_loss: 0.3668 - val_accuracy: 0.9261 - val_recall_m: 0.9213 - val_precision_m: 0.9288 - val_f1_m: 0.9250\n",
      "Epoch 49/300\n",
      "116/155 [=====================>........] - ETA: 0s - loss: 0.2374 - accuracy: 0.9657 - recall_m: 0.9620 - precision_m: 0.9703 - f1_m: 0.9661\n",
      "Epoch 49: val_loss did not improve from 0.34244\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.2369 - accuracy: 0.9656 - recall_m: 0.9616 - precision_m: 0.9697 - f1_m: 0.9656 - val_loss: 0.3710 - val_accuracy: 0.9210 - val_recall_m: 0.9177 - val_precision_m: 0.9240 - val_f1_m: 0.9208\n",
      "Epoch 50/300\n",
      "146/155 [===========================>..] - ETA: 0s - loss: 0.2356 - accuracy: 0.9647 - recall_m: 0.9608 - precision_m: 0.9685 - f1_m: 0.9646\n",
      "Epoch 50: val_loss did not improve from 0.34244\n",
      "155/155 [==============================] - 0s 870us/step - loss: 0.2351 - accuracy: 0.9652 - recall_m: 0.9611 - precision_m: 0.9689 - f1_m: 0.9650 - val_loss: 0.3544 - val_accuracy: 0.9249 - val_recall_m: 0.9228 - val_precision_m: 0.9305 - val_f1_m: 0.9266\n",
      "Epoch 51/300\n",
      "131/155 [========================>.....] - ETA: 0s - loss: 0.2369 - accuracy: 0.9643 - recall_m: 0.9591 - precision_m: 0.9688 - f1_m: 0.9639\n",
      "Epoch 51: val_loss did not improve from 0.34244\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.2367 - accuracy: 0.9641 - recall_m: 0.9595 - precision_m: 0.9688 - f1_m: 0.9641 - val_loss: 0.3772 - val_accuracy: 0.9204 - val_recall_m: 0.9160 - val_precision_m: 0.9263 - val_f1_m: 0.9211\n",
      "Epoch 52/300\n",
      "143/155 [==========================>...] - ETA: 0s - loss: 0.2317 - accuracy: 0.9638 - recall_m: 0.9602 - precision_m: 0.9668 - f1_m: 0.9635\n",
      "Epoch 52: val_loss improved from 0.34244 to 0.33972, saving model to trained_models/fall_model_five_class.h5\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.2343 - accuracy: 0.9634 - recall_m: 0.9597 - precision_m: 0.9664 - f1_m: 0.9630 - val_loss: 0.3397 - val_accuracy: 0.9304 - val_recall_m: 0.9264 - val_precision_m: 0.9336 - val_f1_m: 0.9300\n",
      "Epoch 53/300\n",
      "151/155 [============================>.] - ETA: 0s - loss: 0.2400 - accuracy: 0.9641 - recall_m: 0.9609 - precision_m: 0.9680 - f1_m: 0.9644\n",
      "Epoch 53: val_loss improved from 0.33972 to 0.33872, saving model to trained_models/fall_model_five_class.h5\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.2395 - accuracy: 0.9639 - recall_m: 0.9610 - precision_m: 0.9680 - f1_m: 0.9645 - val_loss: 0.3387 - val_accuracy: 0.9274 - val_recall_m: 0.9253 - val_precision_m: 0.9334 - val_f1_m: 0.9293\n",
      "Epoch 54/300\n",
      "125/155 [=======================>......] - ETA: 0s - loss: 0.2301 - accuracy: 0.9676 - recall_m: 0.9634 - precision_m: 0.9713 - f1_m: 0.9673\n",
      "Epoch 54: val_loss did not improve from 0.33872\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.2296 - accuracy: 0.9677 - recall_m: 0.9643 - precision_m: 0.9718 - f1_m: 0.9680 - val_loss: 0.3919 - val_accuracy: 0.9191 - val_recall_m: 0.9177 - val_precision_m: 0.9227 - val_f1_m: 0.9202\n",
      "Epoch 55/300\n",
      "153/155 [============================>.] - ETA: 0s - loss: 0.2305 - accuracy: 0.9658 - recall_m: 0.9622 - precision_m: 0.9710 - f1_m: 0.9665\n",
      "Epoch 55: val_loss did not improve from 0.33872\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.2309 - accuracy: 0.9656 - recall_m: 0.9611 - precision_m: 0.9698 - f1_m: 0.9654 - val_loss: 0.3507 - val_accuracy: 0.9225 - val_recall_m: 0.9180 - val_precision_m: 0.9269 - val_f1_m: 0.9224\n",
      "Epoch 56/300\n",
      "150/155 [============================>.] - ETA: 0s - loss: 0.2259 - accuracy: 0.9679 - recall_m: 0.9639 - precision_m: 0.9725 - f1_m: 0.9681\n",
      "Epoch 56: val_loss improved from 0.33872 to 0.33751, saving model to trained_models/fall_model_five_class.h5\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.2283 - accuracy: 0.9676 - recall_m: 0.9635 - precision_m: 0.9725 - f1_m: 0.9679 - val_loss: 0.3375 - val_accuracy: 0.9258 - val_recall_m: 0.9222 - val_precision_m: 0.9302 - val_f1_m: 0.9262\n",
      "Epoch 57/300\n",
      " 81/155 [==============>...............] - ETA: 0s - loss: 0.2289 - accuracy: 0.9672 - recall_m: 0.9643 - precision_m: 0.9708 - f1_m: 0.9675\n",
      "Epoch 57: val_loss did not improve from 0.33751\n",
      "155/155 [==============================] - 0s 856us/step - loss: 0.2279 - accuracy: 0.9675 - recall_m: 0.9642 - precision_m: 0.9710 - f1_m: 0.9676 - val_loss: 0.3525 - val_accuracy: 0.9255 - val_recall_m: 0.9229 - val_precision_m: 0.9296 - val_f1_m: 0.9262\n",
      "Epoch 58/300\n",
      "133/155 [========================>.....] - ETA: 0s - loss: 0.2332 - accuracy: 0.9655 - recall_m: 0.9624 - precision_m: 0.9693 - f1_m: 0.9658\n",
      "Epoch 58: val_loss did not improve from 0.33751\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.2351 - accuracy: 0.9653 - recall_m: 0.9625 - precision_m: 0.9693 - f1_m: 0.9658 - val_loss: 0.3464 - val_accuracy: 0.9252 - val_recall_m: 0.9216 - val_precision_m: 0.9293 - val_f1_m: 0.9254\n",
      "Epoch 59/300\n",
      "131/155 [========================>.....] - ETA: 0s - loss: 0.2204 - accuracy: 0.9688 - recall_m: 0.9660 - precision_m: 0.9721 - f1_m: 0.9690\n",
      "Epoch 59: val_loss improved from 0.33751 to 0.32716, saving model to trained_models/fall_model_five_class.h5\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.2266 - accuracy: 0.9672 - recall_m: 0.9636 - precision_m: 0.9708 - f1_m: 0.9671 - val_loss: 0.3272 - val_accuracy: 0.9316 - val_recall_m: 0.9285 - val_precision_m: 0.9349 - val_f1_m: 0.9317\n",
      "Epoch 60/300\n",
      "146/155 [===========================>..] - ETA: 0s - loss: 0.2251 - accuracy: 0.9669 - recall_m: 0.9640 - precision_m: 0.9721 - f1_m: 0.9680\n",
      "Epoch 60: val_loss did not improve from 0.32716\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.2259 - accuracy: 0.9666 - recall_m: 0.9636 - precision_m: 0.9714 - f1_m: 0.9675 - val_loss: 0.3770 - val_accuracy: 0.9191 - val_recall_m: 0.9147 - val_precision_m: 0.9230 - val_f1_m: 0.9188\n",
      "Epoch 61/300\n",
      "113/155 [====================>.........] - ETA: 0s - loss: 0.2249 - accuracy: 0.9674 - recall_m: 0.9646 - precision_m: 0.9713 - f1_m: 0.9679\n",
      "Epoch 61: val_loss did not improve from 0.32716\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.2286 - accuracy: 0.9658 - recall_m: 0.9620 - precision_m: 0.9698 - f1_m: 0.9658 - val_loss: 0.3288 - val_accuracy: 0.9261 - val_recall_m: 0.9235 - val_precision_m: 0.9326 - val_f1_m: 0.9280\n",
      "Epoch 62/300\n",
      " 92/155 [================>.............] - ETA: 0s - loss: 0.2297 - accuracy: 0.9691 - recall_m: 0.9643 - precision_m: 0.9731 - f1_m: 0.9687\n",
      "Epoch 62: val_loss did not improve from 0.32716\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.2245 - accuracy: 0.9700 - recall_m: 0.9654 - precision_m: 0.9736 - f1_m: 0.9694 - val_loss: 0.3340 - val_accuracy: 0.9264 - val_recall_m: 0.9228 - val_precision_m: 0.9322 - val_f1_m: 0.9275\n",
      "Epoch 63/300\n",
      "135/155 [=========================>....] - ETA: 0s - loss: 0.2271 - accuracy: 0.9672 - recall_m: 0.9641 - precision_m: 0.9718 - f1_m: 0.9679\n",
      "Epoch 63: val_loss did not improve from 0.32716\n",
      "155/155 [==============================] - 0s 986us/step - loss: 0.2245 - accuracy: 0.9680 - recall_m: 0.9650 - precision_m: 0.9722 - f1_m: 0.9686 - val_loss: 0.3524 - val_accuracy: 0.9258 - val_recall_m: 0.9229 - val_precision_m: 0.9313 - val_f1_m: 0.9271\n",
      "Epoch 64/300\n",
      "100/155 [==================>...........] - ETA: 0s - loss: 0.2179 - accuracy: 0.9688 - recall_m: 0.9656 - precision_m: 0.9718 - f1_m: 0.9687\n",
      "Epoch 64: val_loss did not improve from 0.32716\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.2246 - accuracy: 0.9668 - recall_m: 0.9635 - precision_m: 0.9704 - f1_m: 0.9669 - val_loss: 0.3811 - val_accuracy: 0.9191 - val_recall_m: 0.9184 - val_precision_m: 0.9251 - val_f1_m: 0.9217\n",
      "Epoch 65/300\n",
      "151/155 [============================>.] - ETA: 0s - loss: 0.2226 - accuracy: 0.9671 - recall_m: 0.9645 - precision_m: 0.9704 - f1_m: 0.9674\n",
      "Epoch 65: val_loss improved from 0.32716 to 0.31159, saving model to trained_models/fall_model_five_class.h5\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.2225 - accuracy: 0.9672 - recall_m: 0.9645 - precision_m: 0.9703 - f1_m: 0.9674 - val_loss: 0.3116 - val_accuracy: 0.9371 - val_recall_m: 0.9300 - val_precision_m: 0.9447 - val_f1_m: 0.9373\n",
      "Epoch 66/300\n",
      "148/155 [===========================>..] - ETA: 0s - loss: 0.2209 - accuracy: 0.9690 - recall_m: 0.9656 - precision_m: 0.9729 - f1_m: 0.9692\n",
      "Epoch 66: val_loss did not improve from 0.31159\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.2212 - accuracy: 0.9689 - recall_m: 0.9657 - precision_m: 0.9728 - f1_m: 0.9692 - val_loss: 0.3561 - val_accuracy: 0.9234 - val_recall_m: 0.9198 - val_precision_m: 0.9276 - val_f1_m: 0.9237\n",
      "Epoch 67/300\n",
      "140/155 [==========================>...] - ETA: 0s - loss: 0.2177 - accuracy: 0.9695 - recall_m: 0.9662 - precision_m: 0.9730 - f1_m: 0.9696\n",
      "Epoch 67: val_loss did not improve from 0.31159\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.2180 - accuracy: 0.9696 - recall_m: 0.9660 - precision_m: 0.9734 - f1_m: 0.9697 - val_loss: 0.3290 - val_accuracy: 0.9261 - val_recall_m: 0.9222 - val_precision_m: 0.9313 - val_f1_m: 0.9267\n",
      "Epoch 68/300\n",
      "104/155 [===================>..........] - ETA: 0s - loss: 0.2182 - accuracy: 0.9683 - recall_m: 0.9650 - precision_m: 0.9717 - f1_m: 0.9683\n",
      "Epoch 68: val_loss did not improve from 0.31159\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.2142 - accuracy: 0.9687 - recall_m: 0.9654 - precision_m: 0.9722 - f1_m: 0.9688 - val_loss: 0.3287 - val_accuracy: 0.9261 - val_recall_m: 0.9213 - val_precision_m: 0.9316 - val_f1_m: 0.9264\n",
      "Epoch 69/300\n",
      "144/155 [==========================>...] - ETA: 0s - loss: 0.2134 - accuracy: 0.9694 - recall_m: 0.9651 - precision_m: 0.9735 - f1_m: 0.9692\n",
      "Epoch 69: val_loss did not improve from 0.31159\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.2151 - accuracy: 0.9685 - recall_m: 0.9646 - precision_m: 0.9729 - f1_m: 0.9687 - val_loss: 0.3586 - val_accuracy: 0.9213 - val_recall_m: 0.9174 - val_precision_m: 0.9261 - val_f1_m: 0.9217\n",
      "Epoch 70/300\n",
      "154/155 [============================>.] - ETA: 0s - loss: 0.2178 - accuracy: 0.9694 - recall_m: 0.9658 - precision_m: 0.9730 - f1_m: 0.9694\n",
      "Epoch 70: val_loss did not improve from 0.31159\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.2176 - accuracy: 0.9694 - recall_m: 0.9660 - precision_m: 0.9732 - f1_m: 0.9696 - val_loss: 0.3118 - val_accuracy: 0.9331 - val_recall_m: 0.9297 - val_precision_m: 0.9398 - val_f1_m: 0.9347\n",
      "Epoch 71/300\n",
      "142/155 [==========================>...] - ETA: 0s - loss: 0.2220 - accuracy: 0.9674 - recall_m: 0.9628 - precision_m: 0.9712 - f1_m: 0.9670\n",
      "Epoch 71: val_loss did not improve from 0.31159\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.2218 - accuracy: 0.9675 - recall_m: 0.9632 - precision_m: 0.9715 - f1_m: 0.9673 - val_loss: 0.3199 - val_accuracy: 0.9267 - val_recall_m: 0.9220 - val_precision_m: 0.9325 - val_f1_m: 0.9272\n",
      "Epoch 72/300\n",
      "143/155 [==========================>...] - ETA: 0s - loss: 0.2173 - accuracy: 0.9683 - recall_m: 0.9655 - precision_m: 0.9730 - f1_m: 0.9692\n",
      "Epoch 72: val_loss did not improve from 0.31159\n",
      "155/155 [==============================] - 0s 988us/step - loss: 0.2177 - accuracy: 0.9683 - recall_m: 0.9657 - precision_m: 0.9730 - f1_m: 0.9693 - val_loss: 0.3399 - val_accuracy: 0.9210 - val_recall_m: 0.9186 - val_precision_m: 0.9269 - val_f1_m: 0.9227\n",
      "Epoch 73/300\n",
      "146/155 [===========================>..] - ETA: 0s - loss: 0.2155 - accuracy: 0.9689 - recall_m: 0.9653 - precision_m: 0.9717 - f1_m: 0.9685\n",
      "Epoch 73: val_loss did not improve from 0.31159\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.2143 - accuracy: 0.9694 - recall_m: 0.9660 - precision_m: 0.9724 - f1_m: 0.9692 - val_loss: 0.3766 - val_accuracy: 0.9231 - val_recall_m: 0.9198 - val_precision_m: 0.9275 - val_f1_m: 0.9236\n",
      "Epoch 74/300\n",
      "142/155 [==========================>...] - ETA: 0s - loss: 0.2203 - accuracy: 0.9678 - recall_m: 0.9642 - precision_m: 0.9719 - f1_m: 0.9680\n",
      "Epoch 74: val_loss did not improve from 0.31159\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.2183 - accuracy: 0.9683 - recall_m: 0.9647 - precision_m: 0.9726 - f1_m: 0.9686 - val_loss: 0.3422 - val_accuracy: 0.9277 - val_recall_m: 0.9237 - val_precision_m: 0.9301 - val_f1_m: 0.9269\n",
      "Epoch 75/300\n",
      "122/155 [======================>.......] - ETA: 0s - loss: 0.2141 - accuracy: 0.9708 - recall_m: 0.9672 - precision_m: 0.9738 - f1_m: 0.9705\n",
      "Epoch 75: val_loss improved from 0.31159 to 0.31058, saving model to trained_models/fall_model_five_class.h5\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.2113 - accuracy: 0.9712 - recall_m: 0.9680 - precision_m: 0.9748 - f1_m: 0.9714 - val_loss: 0.3106 - val_accuracy: 0.9334 - val_recall_m: 0.9288 - val_precision_m: 0.9382 - val_f1_m: 0.9334\n",
      "Epoch 76/300\n",
      "146/155 [===========================>..] - ETA: 0s - loss: 0.2084 - accuracy: 0.9691 - recall_m: 0.9646 - precision_m: 0.9731 - f1_m: 0.9688\n",
      "Epoch 76: val_loss did not improve from 0.31058\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.2095 - accuracy: 0.9689 - recall_m: 0.9647 - precision_m: 0.9730 - f1_m: 0.9688 - val_loss: 0.3481 - val_accuracy: 0.9249 - val_recall_m: 0.9195 - val_precision_m: 0.9292 - val_f1_m: 0.9243\n",
      "Epoch 77/300\n",
      "127/155 [=======================>......] - ETA: 0s - loss: 0.2149 - accuracy: 0.9668 - recall_m: 0.9646 - precision_m: 0.9714 - f1_m: 0.9680\n",
      "Epoch 77: val_loss did not improve from 0.31058\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.2146 - accuracy: 0.9675 - recall_m: 0.9654 - precision_m: 0.9720 - f1_m: 0.9687 - val_loss: 0.3149 - val_accuracy: 0.9298 - val_recall_m: 0.9282 - val_precision_m: 0.9357 - val_f1_m: 0.9319\n",
      "Epoch 78/300\n",
      "141/155 [==========================>...] - ETA: 0s - loss: 0.2110 - accuracy: 0.9704 - recall_m: 0.9674 - precision_m: 0.9741 - f1_m: 0.9707\n",
      "Epoch 78: val_loss did not improve from 0.31058\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.2117 - accuracy: 0.9704 - recall_m: 0.9677 - precision_m: 0.9740 - f1_m: 0.9708 - val_loss: 0.3245 - val_accuracy: 0.9225 - val_recall_m: 0.9187 - val_precision_m: 0.9267 - val_f1_m: 0.9227\n",
      "Epoch 79/300\n",
      "125/155 [=======================>......] - ETA: 0s - loss: 0.2228 - accuracy: 0.9689 - recall_m: 0.9657 - precision_m: 0.9717 - f1_m: 0.9687\n",
      "Epoch 79: val_loss did not improve from 0.31058\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.2199 - accuracy: 0.9693 - recall_m: 0.9661 - precision_m: 0.9724 - f1_m: 0.9692 - val_loss: 0.3986 - val_accuracy: 0.9146 - val_recall_m: 0.9135 - val_precision_m: 0.9199 - val_f1_m: 0.9166\n",
      "Epoch 80/300\n",
      "133/155 [========================>.....] - ETA: 0s - loss: 0.2120 - accuracy: 0.9690 - recall_m: 0.9649 - precision_m: 0.9727 - f1_m: 0.9687\n",
      "Epoch 80: val_loss did not improve from 0.31058\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.2121 - accuracy: 0.9683 - recall_m: 0.9647 - precision_m: 0.9717 - f1_m: 0.9682 - val_loss: 0.3217 - val_accuracy: 0.9328 - val_recall_m: 0.9270 - val_precision_m: 0.9382 - val_f1_m: 0.9325\n",
      "Epoch 81/300\n",
      "126/155 [=======================>......] - ETA: 0s - loss: 0.2066 - accuracy: 0.9689 - recall_m: 0.9659 - precision_m: 0.9730 - f1_m: 0.9694\n",
      "Epoch 81: val_loss did not improve from 0.31058\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.2056 - accuracy: 0.9697 - recall_m: 0.9668 - precision_m: 0.9736 - f1_m: 0.9702 - val_loss: 0.3446 - val_accuracy: 0.9213 - val_recall_m: 0.9180 - val_precision_m: 0.9263 - val_f1_m: 0.9221\n",
      "Epoch 82/300\n",
      "131/155 [========================>.....] - ETA: 0s - loss: 0.2079 - accuracy: 0.9697 - recall_m: 0.9674 - precision_m: 0.9736 - f1_m: 0.9705\n",
      "Epoch 82: val_loss did not improve from 0.31058\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.2098 - accuracy: 0.9688 - recall_m: 0.9658 - precision_m: 0.9728 - f1_m: 0.9693 - val_loss: 0.3214 - val_accuracy: 0.9243 - val_recall_m: 0.9192 - val_precision_m: 0.9334 - val_f1_m: 0.9262\n",
      "Epoch 83/300\n",
      "149/155 [===========================>..] - ETA: 0s - loss: 0.2064 - accuracy: 0.9708 - recall_m: 0.9677 - precision_m: 0.9748 - f1_m: 0.9712\n",
      "Epoch 83: val_loss did not improve from 0.31058\n",
      "155/155 [==============================] - 0s 864us/step - loss: 0.2071 - accuracy: 0.9705 - recall_m: 0.9673 - precision_m: 0.9746 - f1_m: 0.9709 - val_loss: 0.3203 - val_accuracy: 0.9264 - val_recall_m: 0.9244 - val_precision_m: 0.9334 - val_f1_m: 0.9288\n",
      "Epoch 84/300\n",
      " 76/155 [=============>................] - ETA: 0s - loss: 0.2092 - accuracy: 0.9665 - recall_m: 0.9630 - precision_m: 0.9720 - f1_m: 0.9674\n",
      "Epoch 84: val_loss did not improve from 0.31058\n",
      "155/155 [==============================] - 0s 829us/step - loss: 0.2114 - accuracy: 0.9663 - recall_m: 0.9629 - precision_m: 0.9706 - f1_m: 0.9667 - val_loss: 0.3144 - val_accuracy: 0.9347 - val_recall_m: 0.9302 - val_precision_m: 0.9407 - val_f1_m: 0.9354\n",
      "Epoch 85/300\n",
      "127/155 [=======================>......] - ETA: 0s - loss: 0.2109 - accuracy: 0.9686 - recall_m: 0.9658 - precision_m: 0.9724 - f1_m: 0.9690\n",
      "Epoch 85: val_loss did not improve from 0.31058\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.2102 - accuracy: 0.9682 - recall_m: 0.9650 - precision_m: 0.9719 - f1_m: 0.9684 - val_loss: 0.3218 - val_accuracy: 0.9277 - val_recall_m: 0.9246 - val_precision_m: 0.9342 - val_f1_m: 0.9293\n",
      "Epoch 86/300\n",
      "117/155 [=====================>........] - ETA: 0s - loss: 0.2084 - accuracy: 0.9705 - recall_m: 0.9671 - precision_m: 0.9742 - f1_m: 0.9706\n",
      "Epoch 86: val_loss improved from 0.31058 to 0.30640, saving model to trained_models/fall_model_five_class.h5\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.2104 - accuracy: 0.9684 - recall_m: 0.9649 - precision_m: 0.9723 - f1_m: 0.9686 - val_loss: 0.3064 - val_accuracy: 0.9389 - val_recall_m: 0.9359 - val_precision_m: 0.9441 - val_f1_m: 0.9399\n",
      "Epoch 87/300\n",
      "140/155 [==========================>...] - ETA: 0s - loss: 0.2169 - accuracy: 0.9696 - recall_m: 0.9658 - precision_m: 0.9727 - f1_m: 0.9692\n",
      "Epoch 87: val_loss did not improve from 0.30640\n",
      "155/155 [==============================] - 0s 901us/step - loss: 0.2174 - accuracy: 0.9691 - recall_m: 0.9654 - precision_m: 0.9725 - f1_m: 0.9689 - val_loss: 0.3552 - val_accuracy: 0.9267 - val_recall_m: 0.9230 - val_precision_m: 0.9308 - val_f1_m: 0.9269\n",
      "Epoch 88/300\n",
      "130/155 [========================>.....] - ETA: 0s - loss: 0.2094 - accuracy: 0.9694 - recall_m: 0.9661 - precision_m: 0.9731 - f1_m: 0.9696\n",
      "Epoch 88: val_loss did not improve from 0.30640\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.2091 - accuracy: 0.9693 - recall_m: 0.9663 - precision_m: 0.9731 - f1_m: 0.9697 - val_loss: 0.3431 - val_accuracy: 0.9252 - val_recall_m: 0.9219 - val_precision_m: 0.9306 - val_f1_m: 0.9262\n",
      "Epoch 89/300\n",
      "153/155 [============================>.] - ETA: 0s - loss: 0.2024 - accuracy: 0.9704 - recall_m: 0.9680 - precision_m: 0.9744 - f1_m: 0.9712\n",
      "Epoch 89: val_loss did not improve from 0.30640\n",
      "155/155 [==============================] - 0s 866us/step - loss: 0.2023 - accuracy: 0.9704 - recall_m: 0.9682 - precision_m: 0.9745 - f1_m: 0.9714 - val_loss: 0.3524 - val_accuracy: 0.9264 - val_recall_m: 0.9231 - val_precision_m: 0.9291 - val_f1_m: 0.9261\n",
      "Epoch 90/300\n",
      "139/155 [=========================>....] - ETA: 0s - loss: 0.2060 - accuracy: 0.9692 - recall_m: 0.9666 - precision_m: 0.9730 - f1_m: 0.9698\n",
      "Epoch 90: val_loss did not improve from 0.30640\n",
      "155/155 [==============================] - 0s 956us/step - loss: 0.2051 - accuracy: 0.9697 - recall_m: 0.9674 - precision_m: 0.9735 - f1_m: 0.9704 - val_loss: 0.3316 - val_accuracy: 0.9286 - val_recall_m: 0.9283 - val_precision_m: 0.9314 - val_f1_m: 0.9299\n",
      "Epoch 91/300\n",
      "115/155 [=====================>........] - ETA: 0s - loss: 0.2112 - accuracy: 0.9712 - recall_m: 0.9673 - precision_m: 0.9748 - f1_m: 0.9710\n",
      "Epoch 91: val_loss improved from 0.30640 to 0.30634, saving model to trained_models/fall_model_five_class.h5\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.2107 - accuracy: 0.9702 - recall_m: 0.9661 - precision_m: 0.9737 - f1_m: 0.9699 - val_loss: 0.3063 - val_accuracy: 0.9298 - val_recall_m: 0.9241 - val_precision_m: 0.9339 - val_f1_m: 0.9289\n",
      "Epoch 92/300\n",
      "109/155 [====================>.........] - ETA: 0s - loss: 0.2084 - accuracy: 0.9689 - recall_m: 0.9662 - precision_m: 0.9733 - f1_m: 0.9697\n",
      "Epoch 92: val_loss did not improve from 0.30634\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.2074 - accuracy: 0.9695 - recall_m: 0.9671 - precision_m: 0.9740 - f1_m: 0.9705 - val_loss: 0.3371 - val_accuracy: 0.9258 - val_recall_m: 0.9222 - val_precision_m: 0.9322 - val_f1_m: 0.9272\n",
      "Epoch 93/300\n",
      "141/155 [==========================>...] - ETA: 0s - loss: 0.2021 - accuracy: 0.9719 - recall_m: 0.9693 - precision_m: 0.9749 - f1_m: 0.9721\n",
      "Epoch 93: val_loss did not improve from 0.30634\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.2022 - accuracy: 0.9720 - recall_m: 0.9699 - precision_m: 0.9754 - f1_m: 0.9726 - val_loss: 0.3255 - val_accuracy: 0.9325 - val_recall_m: 0.9288 - val_precision_m: 0.9357 - val_f1_m: 0.9322\n",
      "Epoch 94/300\n",
      "146/155 [===========================>..] - ETA: 0s - loss: 0.2035 - accuracy: 0.9715 - recall_m: 0.9684 - precision_m: 0.9747 - f1_m: 0.9715\n",
      "Epoch 94: val_loss did not improve from 0.30634\n",
      "155/155 [==============================] - 0s 3ms/step - loss: 0.2018 - accuracy: 0.9722 - recall_m: 0.9694 - precision_m: 0.9755 - f1_m: 0.9724 - val_loss: 0.3126 - val_accuracy: 0.9389 - val_recall_m: 0.9362 - val_precision_m: 0.9416 - val_f1_m: 0.9388\n",
      "Epoch 95/300\n",
      "135/155 [=========================>....] - ETA: 0s - loss: 0.2031 - accuracy: 0.9708 - recall_m: 0.9683 - precision_m: 0.9744 - f1_m: 0.9713\n",
      "Epoch 95: val_loss did not improve from 0.30634\n",
      "155/155 [==============================] - 0s 969us/step - loss: 0.2019 - accuracy: 0.9705 - recall_m: 0.9683 - precision_m: 0.9745 - f1_m: 0.9714 - val_loss: 0.3135 - val_accuracy: 0.9255 - val_recall_m: 0.9215 - val_precision_m: 0.9302 - val_f1_m: 0.9258\n",
      "Epoch 96/300\n",
      "145/155 [===========================>..] - ETA: 0s - loss: 0.2039 - accuracy: 0.9689 - recall_m: 0.9664 - precision_m: 0.9728 - f1_m: 0.9695\n",
      "Epoch 96: val_loss did not improve from 0.30634\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.2040 - accuracy: 0.9687 - recall_m: 0.9663 - precision_m: 0.9727 - f1_m: 0.9695 - val_loss: 0.3624 - val_accuracy: 0.9131 - val_recall_m: 0.9099 - val_precision_m: 0.9209 - val_f1_m: 0.9153\n",
      "Epoch 97/300\n",
      "120/155 [======================>.......] - ETA: 0s - loss: 0.1948 - accuracy: 0.9721 - recall_m: 0.9697 - precision_m: 0.9751 - f1_m: 0.9724\n",
      "Epoch 97: val_loss did not improve from 0.30634\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.1966 - accuracy: 0.9719 - recall_m: 0.9691 - precision_m: 0.9752 - f1_m: 0.9721 - val_loss: 0.3494 - val_accuracy: 0.9261 - val_recall_m: 0.9243 - val_precision_m: 0.9315 - val_f1_m: 0.9279\n",
      "Epoch 98/300\n",
      " 99/155 [==================>...........] - ETA: 0s - loss: 0.2086 - accuracy: 0.9714 - recall_m: 0.9686 - precision_m: 0.9734 - f1_m: 0.9709\n",
      "Epoch 98: val_loss improved from 0.30634 to 0.30296, saving model to trained_models/fall_model_five_class.h5\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.2062 - accuracy: 0.9710 - recall_m: 0.9684 - precision_m: 0.9737 - f1_m: 0.9710 - val_loss: 0.3030 - val_accuracy: 0.9374 - val_recall_m: 0.9314 - val_precision_m: 0.9407 - val_f1_m: 0.9360\n",
      "Epoch 99/300\n",
      "143/155 [==========================>...] - ETA: 0s - loss: 0.2016 - accuracy: 0.9714 - recall_m: 0.9693 - precision_m: 0.9738 - f1_m: 0.9715\n",
      "Epoch 99: val_loss improved from 0.30296 to 0.29688, saving model to trained_models/fall_model_five_class.h5\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.2032 - accuracy: 0.9708 - recall_m: 0.9689 - precision_m: 0.9737 - f1_m: 0.9712 - val_loss: 0.2969 - val_accuracy: 0.9371 - val_recall_m: 0.9324 - val_precision_m: 0.9399 - val_f1_m: 0.9361\n",
      "Epoch 100/300\n",
      "141/155 [==========================>...] - ETA: 0s - loss: 0.1999 - accuracy: 0.9726 - recall_m: 0.9704 - precision_m: 0.9767 - f1_m: 0.9735\n",
      "Epoch 100: val_loss did not improve from 0.29688\n",
      "155/155 [==============================] - 0s 899us/step - loss: 0.1999 - accuracy: 0.9726 - recall_m: 0.9702 - precision_m: 0.9768 - f1_m: 0.9735 - val_loss: 0.3229 - val_accuracy: 0.9334 - val_recall_m: 0.9303 - val_precision_m: 0.9358 - val_f1_m: 0.9330\n",
      "Epoch 101/300\n",
      "136/155 [=========================>....] - ETA: 0s - loss: 0.1970 - accuracy: 0.9706 - recall_m: 0.9674 - precision_m: 0.9748 - f1_m: 0.9710\n",
      "Epoch 101: val_loss did not improve from 0.29688\n",
      "155/155 [==============================] - 0s 926us/step - loss: 0.1987 - accuracy: 0.9698 - recall_m: 0.9665 - precision_m: 0.9743 - f1_m: 0.9703 - val_loss: 0.3230 - val_accuracy: 0.9246 - val_recall_m: 0.9198 - val_precision_m: 0.9309 - val_f1_m: 0.9253\n",
      "Epoch 102/300\n",
      "142/155 [==========================>...] - ETA: 0s - loss: 0.2044 - accuracy: 0.9672 - recall_m: 0.9647 - precision_m: 0.9718 - f1_m: 0.9682\n",
      "Epoch 102: val_loss did not improve from 0.29688\n",
      "155/155 [==============================] - 0s 911us/step - loss: 0.2048 - accuracy: 0.9674 - recall_m: 0.9644 - precision_m: 0.9717 - f1_m: 0.9680 - val_loss: 0.3278 - val_accuracy: 0.9283 - val_recall_m: 0.9246 - val_precision_m: 0.9329 - val_f1_m: 0.9287\n",
      "Epoch 103/300\n",
      "152/155 [============================>.] - ETA: 0s - loss: 0.2029 - accuracy: 0.9707 - recall_m: 0.9680 - precision_m: 0.9743 - f1_m: 0.9711\n",
      "Epoch 103: val_loss did not improve from 0.29688\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.2034 - accuracy: 0.9705 - recall_m: 0.9677 - precision_m: 0.9739 - f1_m: 0.9708 - val_loss: 0.3261 - val_accuracy: 0.9292 - val_recall_m: 0.9264 - val_precision_m: 0.9325 - val_f1_m: 0.9294\n",
      "Epoch 104/300\n",
      "103/155 [==================>...........] - ETA: 0s - loss: 0.2013 - accuracy: 0.9710 - recall_m: 0.9675 - precision_m: 0.9748 - f1_m: 0.9711\n",
      "Epoch 104: val_loss did not improve from 0.29688\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.2048 - accuracy: 0.9709 - recall_m: 0.9679 - precision_m: 0.9743 - f1_m: 0.9711 - val_loss: 0.3100 - val_accuracy: 0.9310 - val_recall_m: 0.9282 - val_precision_m: 0.9348 - val_f1_m: 0.9315\n",
      "Epoch 105/300\n",
      "146/155 [===========================>..] - ETA: 0s - loss: 0.2017 - accuracy: 0.9705 - recall_m: 0.9678 - precision_m: 0.9740 - f1_m: 0.9709\n",
      "Epoch 105: val_loss did not improve from 0.29688\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.1998 - accuracy: 0.9711 - recall_m: 0.9685 - precision_m: 0.9748 - f1_m: 0.9717 - val_loss: 0.3384 - val_accuracy: 0.9301 - val_recall_m: 0.9267 - val_precision_m: 0.9335 - val_f1_m: 0.9301\n",
      "Epoch 106/300\n",
      "144/155 [==========================>...] - ETA: 0s - loss: 0.2010 - accuracy: 0.9723 - recall_m: 0.9695 - precision_m: 0.9748 - f1_m: 0.9721\n",
      "Epoch 106: val_loss did not improve from 0.29688\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.2016 - accuracy: 0.9722 - recall_m: 0.9689 - precision_m: 0.9743 - f1_m: 0.9716 - val_loss: 0.3291 - val_accuracy: 0.9258 - val_recall_m: 0.9209 - val_precision_m: 0.9300 - val_f1_m: 0.9254\n",
      "Epoch 107/300\n",
      "148/155 [===========================>..] - ETA: 0s - loss: 0.2016 - accuracy: 0.9707 - recall_m: 0.9673 - precision_m: 0.9735 - f1_m: 0.9704\n",
      "Epoch 107: val_loss did not improve from 0.29688\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.2017 - accuracy: 0.9707 - recall_m: 0.9671 - precision_m: 0.9736 - f1_m: 0.9703 - val_loss: 0.3337 - val_accuracy: 0.9249 - val_recall_m: 0.9208 - val_precision_m: 0.9305 - val_f1_m: 0.9256\n",
      "Epoch 108/300\n",
      "154/155 [============================>.] - ETA: 0s - loss: 0.2000 - accuracy: 0.9722 - recall_m: 0.9694 - precision_m: 0.9758 - f1_m: 0.9725\n",
      "Epoch 108: val_loss did not improve from 0.29688\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.2005 - accuracy: 0.9721 - recall_m: 0.9691 - precision_m: 0.9755 - f1_m: 0.9723 - val_loss: 0.3152 - val_accuracy: 0.9252 - val_recall_m: 0.9225 - val_precision_m: 0.9302 - val_f1_m: 0.9263\n",
      "Epoch 109/300\n",
      "142/155 [==========================>...] - ETA: 0s - loss: 0.1973 - accuracy: 0.9719 - recall_m: 0.9688 - precision_m: 0.9760 - f1_m: 0.9724\n",
      "Epoch 109: val_loss did not improve from 0.29688\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.1978 - accuracy: 0.9720 - recall_m: 0.9687 - precision_m: 0.9760 - f1_m: 0.9723 - val_loss: 0.3411 - val_accuracy: 0.9216 - val_recall_m: 0.9179 - val_precision_m: 0.9258 - val_f1_m: 0.9218\n",
      "Epoch 110/300\n",
      "136/155 [=========================>....] - ETA: 0s - loss: 0.2003 - accuracy: 0.9715 - recall_m: 0.9678 - precision_m: 0.9747 - f1_m: 0.9712\n",
      "Epoch 110: val_loss did not improve from 0.29688\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.2017 - accuracy: 0.9717 - recall_m: 0.9683 - precision_m: 0.9749 - f1_m: 0.9716 - val_loss: 0.3289 - val_accuracy: 0.9292 - val_recall_m: 0.9277 - val_precision_m: 0.9323 - val_f1_m: 0.9300\n",
      "Epoch 111/300\n",
      "148/155 [===========================>..] - ETA: 0s - loss: 0.2028 - accuracy: 0.9695 - recall_m: 0.9671 - precision_m: 0.9722 - f1_m: 0.9696\n",
      "Epoch 111: val_loss did not improve from 0.29688\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.2021 - accuracy: 0.9698 - recall_m: 0.9672 - precision_m: 0.9728 - f1_m: 0.9700 - val_loss: 0.3104 - val_accuracy: 0.9292 - val_recall_m: 0.9252 - val_precision_m: 0.9334 - val_f1_m: 0.9292\n",
      "Epoch 112/300\n",
      "141/155 [==========================>...] - ETA: 0s - loss: 0.1932 - accuracy: 0.9720 - recall_m: 0.9686 - precision_m: 0.9746 - f1_m: 0.9716\n",
      "Epoch 112: val_loss did not improve from 0.29688\n",
      "155/155 [==============================] - 0s 896us/step - loss: 0.1954 - accuracy: 0.9712 - recall_m: 0.9681 - precision_m: 0.9740 - f1_m: 0.9711 - val_loss: 0.3680 - val_accuracy: 0.9188 - val_recall_m: 0.9160 - val_precision_m: 0.9230 - val_f1_m: 0.9195\n",
      "Epoch 113/300\n",
      " 78/155 [==============>...............] - ETA: 0s - loss: 0.1995 - accuracy: 0.9698 - recall_m: 0.9663 - precision_m: 0.9738 - f1_m: 0.9700\n",
      "Epoch 113: val_loss did not improve from 0.29688\n",
      "155/155 [==============================] - 0s 819us/step - loss: 0.2009 - accuracy: 0.9696 - recall_m: 0.9666 - precision_m: 0.9737 - f1_m: 0.9701 - val_loss: 0.3320 - val_accuracy: 0.9252 - val_recall_m: 0.9213 - val_precision_m: 0.9313 - val_f1_m: 0.9262\n",
      "Epoch 114/300\n",
      "140/155 [==========================>...] - ETA: 0s - loss: 0.1989 - accuracy: 0.9696 - recall_m: 0.9670 - precision_m: 0.9727 - f1_m: 0.9698\n",
      "Epoch 114: val_loss did not improve from 0.29688\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.1978 - accuracy: 0.9702 - recall_m: 0.9675 - precision_m: 0.9733 - f1_m: 0.9704 - val_loss: 0.3313 - val_accuracy: 0.9222 - val_recall_m: 0.9192 - val_precision_m: 0.9265 - val_f1_m: 0.9228\n",
      "Epoch 115/300\n",
      "137/155 [=========================>....] - ETA: 0s - loss: 0.1988 - accuracy: 0.9723 - recall_m: 0.9694 - precision_m: 0.9762 - f1_m: 0.9728\n",
      "Epoch 115: val_loss did not improve from 0.29688\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.1972 - accuracy: 0.9727 - recall_m: 0.9703 - precision_m: 0.9768 - f1_m: 0.9735 - val_loss: 0.3219 - val_accuracy: 0.9228 - val_recall_m: 0.9183 - val_precision_m: 0.9277 - val_f1_m: 0.9230\n",
      "Epoch 116/300\n",
      "150/155 [============================>.] - ETA: 0s - loss: 0.1923 - accuracy: 0.9727 - recall_m: 0.9696 - precision_m: 0.9760 - f1_m: 0.9728\n",
      "Epoch 116: val_loss improved from 0.29688 to 0.28623, saving model to trained_models/fall_model_five_class.h5\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.1926 - accuracy: 0.9727 - recall_m: 0.9698 - precision_m: 0.9761 - f1_m: 0.9729 - val_loss: 0.2862 - val_accuracy: 0.9471 - val_recall_m: 0.9447 - val_precision_m: 0.9524 - val_f1_m: 0.9485\n",
      "Epoch 117/300\n",
      "116/155 [=====================>........] - ETA: 0s - loss: 0.2000 - accuracy: 0.9709 - recall_m: 0.9685 - precision_m: 0.9741 - f1_m: 0.9713\n",
      "Epoch 117: val_loss did not improve from 0.28623\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.1990 - accuracy: 0.9719 - recall_m: 0.9690 - precision_m: 0.9747 - f1_m: 0.9718 - val_loss: 0.3247 - val_accuracy: 0.9280 - val_recall_m: 0.9240 - val_precision_m: 0.9315 - val_f1_m: 0.9277\n",
      "Epoch 118/300\n",
      "140/155 [==========================>...] - ETA: 0s - loss: 0.1990 - accuracy: 0.9705 - recall_m: 0.9679 - precision_m: 0.9732 - f1_m: 0.9705\n",
      "Epoch 118: val_loss did not improve from 0.28623\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.1978 - accuracy: 0.9714 - recall_m: 0.9685 - precision_m: 0.9741 - f1_m: 0.9713 - val_loss: 0.3087 - val_accuracy: 0.9292 - val_recall_m: 0.9247 - val_precision_m: 0.9329 - val_f1_m: 0.9288\n",
      "Epoch 119/300\n",
      "112/155 [====================>.........] - ETA: 0s - loss: 0.1861 - accuracy: 0.9746 - recall_m: 0.9717 - precision_m: 0.9772 - f1_m: 0.9744\n",
      "Epoch 119: val_loss did not improve from 0.28623\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.1877 - accuracy: 0.9747 - recall_m: 0.9716 - precision_m: 0.9775 - f1_m: 0.9745 - val_loss: 0.3315 - val_accuracy: 0.9307 - val_recall_m: 0.9278 - val_precision_m: 0.9344 - val_f1_m: 0.9310\n",
      "Epoch 120/300\n",
      "147/155 [===========================>..] - ETA: 0s - loss: 0.1911 - accuracy: 0.9708 - recall_m: 0.9673 - precision_m: 0.9749 - f1_m: 0.9710\n",
      "Epoch 120: val_loss did not improve from 0.28623\n",
      "155/155 [==============================] - 0s 968us/step - loss: 0.1930 - accuracy: 0.9706 - recall_m: 0.9673 - precision_m: 0.9750 - f1_m: 0.9711 - val_loss: 0.3288 - val_accuracy: 0.9234 - val_recall_m: 0.9192 - val_precision_m: 0.9294 - val_f1_m: 0.9242\n",
      "Epoch 121/300\n",
      "142/155 [==========================>...] - ETA: 0s - loss: 0.1956 - accuracy: 0.9725 - recall_m: 0.9697 - precision_m: 0.9754 - f1_m: 0.9726\n",
      "Epoch 121: val_loss did not improve from 0.28623\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.1958 - accuracy: 0.9722 - recall_m: 0.9698 - precision_m: 0.9752 - f1_m: 0.9724 - val_loss: 0.3123 - val_accuracy: 0.9267 - val_recall_m: 0.9246 - val_precision_m: 0.9305 - val_f1_m: 0.9275\n",
      "Epoch 122/300\n",
      "120/155 [======================>.......] - ETA: 0s - loss: 0.1954 - accuracy: 0.9720 - recall_m: 0.9688 - precision_m: 0.9747 - f1_m: 0.9717\n",
      "Epoch 122: val_loss did not improve from 0.28623\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.1974 - accuracy: 0.9715 - recall_m: 0.9683 - precision_m: 0.9741 - f1_m: 0.9711 - val_loss: 0.3175 - val_accuracy: 0.9255 - val_recall_m: 0.9213 - val_precision_m: 0.9324 - val_f1_m: 0.9268\n",
      "Epoch 123/300\n",
      "135/155 [=========================>....] - ETA: 0s - loss: 0.1893 - accuracy: 0.9731 - recall_m: 0.9704 - precision_m: 0.9767 - f1_m: 0.9735\n",
      "Epoch 123: val_loss did not improve from 0.28623\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.1927 - accuracy: 0.9727 - recall_m: 0.9702 - precision_m: 0.9761 - f1_m: 0.9731 - val_loss: 0.3365 - val_accuracy: 0.9179 - val_recall_m: 0.9123 - val_precision_m: 0.9213 - val_f1_m: 0.9168\n",
      "Epoch 124/300\n",
      "154/155 [============================>.] - ETA: 0s - loss: 0.1998 - accuracy: 0.9700 - recall_m: 0.9667 - precision_m: 0.9733 - f1_m: 0.9700\n",
      "Epoch 124: val_loss did not improve from 0.28623\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.1999 - accuracy: 0.9699 - recall_m: 0.9665 - precision_m: 0.9735 - f1_m: 0.9700 - val_loss: 0.2901 - val_accuracy: 0.9401 - val_recall_m: 0.9353 - val_precision_m: 0.9462 - val_f1_m: 0.9406\n",
      "Epoch 125/300\n",
      "115/155 [=====================>........] - ETA: 0s - loss: 0.1868 - accuracy: 0.9740 - recall_m: 0.9713 - precision_m: 0.9772 - f1_m: 0.9742\n",
      "Epoch 125: val_loss improved from 0.28623 to 0.28555, saving model to trained_models/fall_model_five_class.h5\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.1926 - accuracy: 0.9732 - recall_m: 0.9708 - precision_m: 0.9763 - f1_m: 0.9735 - val_loss: 0.2856 - val_accuracy: 0.9416 - val_recall_m: 0.9360 - val_precision_m: 0.9464 - val_f1_m: 0.9411\n",
      "Epoch 126/300\n",
      "128/155 [=======================>......] - ETA: 0s - loss: 0.1910 - accuracy: 0.9711 - recall_m: 0.9679 - precision_m: 0.9743 - f1_m: 0.9711\n",
      "Epoch 126: val_loss did not improve from 0.28555\n",
      "155/155 [==============================] - 0s 985us/step - loss: 0.1929 - accuracy: 0.9704 - recall_m: 0.9676 - precision_m: 0.9738 - f1_m: 0.9707 - val_loss: 0.3024 - val_accuracy: 0.9350 - val_recall_m: 0.9308 - val_precision_m: 0.9376 - val_f1_m: 0.9342\n",
      "Epoch 127/300\n",
      "128/155 [=======================>......] - ETA: 0s - loss: 0.1925 - accuracy: 0.9730 - recall_m: 0.9696 - precision_m: 0.9758 - f1_m: 0.9727\n",
      "Epoch 127: val_loss did not improve from 0.28555\n",
      "155/155 [==============================] - 0s 1000us/step - loss: 0.1915 - accuracy: 0.9732 - recall_m: 0.9700 - precision_m: 0.9763 - f1_m: 0.9731 - val_loss: 0.3625 - val_accuracy: 0.9249 - val_recall_m: 0.9222 - val_precision_m: 0.9291 - val_f1_m: 0.9256\n",
      "Epoch 128/300\n",
      "144/155 [==========================>...] - ETA: 0s - loss: 0.1904 - accuracy: 0.9740 - recall_m: 0.9716 - precision_m: 0.9773 - f1_m: 0.9744\n",
      "Epoch 128: val_loss did not improve from 0.28555\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.1894 - accuracy: 0.9739 - recall_m: 0.9717 - precision_m: 0.9775 - f1_m: 0.9745 - val_loss: 0.3440 - val_accuracy: 0.9161 - val_recall_m: 0.9130 - val_precision_m: 0.9169 - val_f1_m: 0.9150\n",
      "Epoch 129/300\n",
      "155/155 [==============================] - ETA: 0s - loss: 0.1985 - accuracy: 0.9701 - recall_m: 0.9671 - precision_m: 0.9739 - f1_m: 0.9705\n",
      "Epoch 129: val_loss did not improve from 0.28555\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.1985 - accuracy: 0.9701 - recall_m: 0.9671 - precision_m: 0.9739 - f1_m: 0.9705 - val_loss: 0.3215 - val_accuracy: 0.9222 - val_recall_m: 0.9203 - val_precision_m: 0.9284 - val_f1_m: 0.9243\n",
      "Epoch 130/300\n",
      "122/155 [======================>.......] - ETA: 0s - loss: 0.1985 - accuracy: 0.9720 - recall_m: 0.9689 - precision_m: 0.9755 - f1_m: 0.9722\n",
      "Epoch 130: val_loss did not improve from 0.28555\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.1961 - accuracy: 0.9720 - recall_m: 0.9692 - precision_m: 0.9759 - f1_m: 0.9725 - val_loss: 0.3286 - val_accuracy: 0.9289 - val_recall_m: 0.9246 - val_precision_m: 0.9320 - val_f1_m: 0.9283\n",
      "Epoch 131/300\n",
      "130/155 [========================>.....] - ETA: 0s - loss: 0.1907 - accuracy: 0.9724 - recall_m: 0.9694 - precision_m: 0.9745 - f1_m: 0.9719\n",
      "Epoch 131: val_loss did not improve from 0.28555\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.1904 - accuracy: 0.9725 - recall_m: 0.9698 - precision_m: 0.9751 - f1_m: 0.9724 - val_loss: 0.3595 - val_accuracy: 0.9158 - val_recall_m: 0.9087 - val_precision_m: 0.9191 - val_f1_m: 0.9138\n",
      "Epoch 132/300\n",
      "154/155 [============================>.] - ETA: 0s - loss: 0.1951 - accuracy: 0.9713 - recall_m: 0.9681 - precision_m: 0.9749 - f1_m: 0.9715\n",
      "Epoch 132: val_loss did not improve from 0.28555\n",
      "155/155 [==============================] - 0s 845us/step - loss: 0.1951 - accuracy: 0.9713 - recall_m: 0.9683 - precision_m: 0.9750 - f1_m: 0.9716 - val_loss: 0.3304 - val_accuracy: 0.9201 - val_recall_m: 0.9165 - val_precision_m: 0.9253 - val_f1_m: 0.9208\n",
      "Epoch 133/300\n",
      "132/155 [========================>.....] - ETA: 0s - loss: 0.1938 - accuracy: 0.9718 - recall_m: 0.9683 - precision_m: 0.9751 - f1_m: 0.9716\n",
      "Epoch 133: val_loss did not improve from 0.28555\n",
      "155/155 [==============================] - 0s 931us/step - loss: 0.1972 - accuracy: 0.9708 - recall_m: 0.9672 - precision_m: 0.9741 - f1_m: 0.9706 - val_loss: 0.3430 - val_accuracy: 0.9182 - val_recall_m: 0.9138 - val_precision_m: 0.9234 - val_f1_m: 0.9185\n",
      "Epoch 134/300\n",
      "145/155 [===========================>..] - ETA: 0s - loss: 0.1960 - accuracy: 0.9704 - recall_m: 0.9678 - precision_m: 0.9744 - f1_m: 0.9711\n",
      "Epoch 134: val_loss did not improve from 0.28555\n",
      "155/155 [==============================] - 0s 910us/step - loss: 0.1956 - accuracy: 0.9705 - recall_m: 0.9681 - precision_m: 0.9746 - f1_m: 0.9713 - val_loss: 0.3062 - val_accuracy: 0.9264 - val_recall_m: 0.9216 - val_precision_m: 0.9290 - val_f1_m: 0.9253\n",
      "Epoch 135/300\n",
      "154/155 [============================>.] - ETA: 0s - loss: 0.1933 - accuracy: 0.9722 - recall_m: 0.9695 - precision_m: 0.9751 - f1_m: 0.9722\n",
      "Epoch 135: val_loss did not improve from 0.28555\n",
      "155/155 [==============================] - 0s 869us/step - loss: 0.1932 - accuracy: 0.9722 - recall_m: 0.9697 - precision_m: 0.9753 - f1_m: 0.9724 - val_loss: 0.3495 - val_accuracy: 0.9213 - val_recall_m: 0.9186 - val_precision_m: 0.9276 - val_f1_m: 0.9230\n",
      "Epoch 136/300\n",
      "144/155 [==========================>...] - ETA: 0s - loss: 0.1939 - accuracy: 0.9702 - recall_m: 0.9681 - precision_m: 0.9736 - f1_m: 0.9708\n",
      "Epoch 136: val_loss did not improve from 0.28555\n",
      "155/155 [==============================] - 0s 905us/step - loss: 0.1936 - accuracy: 0.9701 - recall_m: 0.9678 - precision_m: 0.9732 - f1_m: 0.9704 - val_loss: 0.3417 - val_accuracy: 0.9198 - val_recall_m: 0.9168 - val_precision_m: 0.9270 - val_f1_m: 0.9218\n",
      "Epoch 137/300\n",
      "143/155 [==========================>...] - ETA: 0s - loss: 0.1980 - accuracy: 0.9704 - recall_m: 0.9682 - precision_m: 0.9736 - f1_m: 0.9709\n",
      "Epoch 137: val_loss did not improve from 0.28555\n",
      "155/155 [==============================] - 0s 914us/step - loss: 0.1976 - accuracy: 0.9703 - recall_m: 0.9683 - precision_m: 0.9738 - f1_m: 0.9711 - val_loss: 0.3443 - val_accuracy: 0.9246 - val_recall_m: 0.9221 - val_precision_m: 0.9298 - val_f1_m: 0.9259\n",
      "Epoch 138/300\n",
      "138/155 [=========================>....] - ETA: 0s - loss: 0.1932 - accuracy: 0.9721 - recall_m: 0.9685 - precision_m: 0.9750 - f1_m: 0.9717\n",
      "Epoch 138: val_loss did not improve from 0.28555\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.1902 - accuracy: 0.9727 - recall_m: 0.9692 - precision_m: 0.9757 - f1_m: 0.9724 - val_loss: 0.3078 - val_accuracy: 0.9213 - val_recall_m: 0.9190 - val_precision_m: 0.9290 - val_f1_m: 0.9240\n",
      "Epoch 139/300\n",
      "146/155 [===========================>..] - ETA: 0s - loss: 0.1864 - accuracy: 0.9735 - recall_m: 0.9712 - precision_m: 0.9770 - f1_m: 0.9741\n",
      "Epoch 139: val_loss did not improve from 0.28555\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.1877 - accuracy: 0.9730 - recall_m: 0.9707 - precision_m: 0.9770 - f1_m: 0.9738 - val_loss: 0.3220 - val_accuracy: 0.9213 - val_recall_m: 0.9156 - val_precision_m: 0.9286 - val_f1_m: 0.9220\n",
      "Epoch 140/300\n",
      "125/155 [=======================>......] - ETA: 0s - loss: 0.1897 - accuracy: 0.9725 - recall_m: 0.9691 - precision_m: 0.9747 - f1_m: 0.9719\n",
      "Epoch 140: val_loss did not improve from 0.28555\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.1890 - accuracy: 0.9723 - recall_m: 0.9694 - precision_m: 0.9746 - f1_m: 0.9719 - val_loss: 0.3154 - val_accuracy: 0.9258 - val_recall_m: 0.9234 - val_precision_m: 0.9290 - val_f1_m: 0.9262\n",
      "Epoch 141/300\n",
      "151/155 [============================>.] - ETA: 0s - loss: 0.1918 - accuracy: 0.9705 - recall_m: 0.9674 - precision_m: 0.9746 - f1_m: 0.9710\n",
      "Epoch 141: val_loss did not improve from 0.28555\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.1915 - accuracy: 0.9706 - recall_m: 0.9677 - precision_m: 0.9747 - f1_m: 0.9712 - val_loss: 0.3390 - val_accuracy: 0.9152 - val_recall_m: 0.9115 - val_precision_m: 0.9205 - val_f1_m: 0.9160\n",
      "Epoch 142/300\n",
      "128/155 [=======================>......] - ETA: 0s - loss: 0.1868 - accuracy: 0.9736 - recall_m: 0.9716 - precision_m: 0.9762 - f1_m: 0.9739\n",
      "Epoch 142: val_loss did not improve from 0.28555\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.1895 - accuracy: 0.9732 - recall_m: 0.9700 - precision_m: 0.9757 - f1_m: 0.9728 - val_loss: 0.3404 - val_accuracy: 0.9128 - val_recall_m: 0.9105 - val_precision_m: 0.9206 - val_f1_m: 0.9155\n",
      "Epoch 143/300\n",
      "150/155 [============================>.] - ETA: 0s - loss: 0.1904 - accuracy: 0.9741 - recall_m: 0.9711 - precision_m: 0.9766 - f1_m: 0.9739\n",
      "Epoch 143: val_loss did not improve from 0.28555\n",
      "155/155 [==============================] - 0s 872us/step - loss: 0.1904 - accuracy: 0.9741 - recall_m: 0.9706 - precision_m: 0.9761 - f1_m: 0.9733 - val_loss: 0.3332 - val_accuracy: 0.9240 - val_recall_m: 0.9214 - val_precision_m: 0.9273 - val_f1_m: 0.9243\n",
      "Epoch 144/300\n",
      "103/155 [==================>...........] - ETA: 0s - loss: 0.2079 - accuracy: 0.9681 - recall_m: 0.9659 - precision_m: 0.9725 - f1_m: 0.9692\n",
      "Epoch 144: val_loss improved from 0.28555 to 0.27442, saving model to trained_models/fall_model_five_class.h5\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.1993 - accuracy: 0.9698 - recall_m: 0.9678 - precision_m: 0.9736 - f1_m: 0.9707 - val_loss: 0.2744 - val_accuracy: 0.9526 - val_recall_m: 0.9486 - val_precision_m: 0.9563 - val_f1_m: 0.9524\n",
      "Epoch 145/300\n",
      "125/155 [=======================>......] - ETA: 0s - loss: 0.2009 - accuracy: 0.9711 - recall_m: 0.9676 - precision_m: 0.9743 - f1_m: 0.9709\n",
      "Epoch 145: val_loss did not improve from 0.27442\n",
      "155/155 [==============================] - 0s 996us/step - loss: 0.1996 - accuracy: 0.9713 - recall_m: 0.9682 - precision_m: 0.9745 - f1_m: 0.9713 - val_loss: 0.2967 - val_accuracy: 0.9368 - val_recall_m: 0.9344 - val_precision_m: 0.9411 - val_f1_m: 0.9377\n",
      "Epoch 146/300\n",
      "134/155 [========================>.....] - ETA: 0s - loss: 0.1966 - accuracy: 0.9708 - recall_m: 0.9676 - precision_m: 0.9736 - f1_m: 0.9706\n",
      "Epoch 146: val_loss did not improve from 0.27442\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.1955 - accuracy: 0.9710 - recall_m: 0.9674 - precision_m: 0.9739 - f1_m: 0.9706 - val_loss: 0.3467 - val_accuracy: 0.9170 - val_recall_m: 0.9118 - val_precision_m: 0.9211 - val_f1_m: 0.9164\n",
      "Epoch 147/300\n",
      "120/155 [======================>.......] - ETA: 0s - loss: 0.1895 - accuracy: 0.9743 - recall_m: 0.9715 - precision_m: 0.9779 - f1_m: 0.9746\n",
      "Epoch 147: val_loss did not improve from 0.27442\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.1900 - accuracy: 0.9736 - recall_m: 0.9705 - precision_m: 0.9765 - f1_m: 0.9735 - val_loss: 0.2962 - val_accuracy: 0.9264 - val_recall_m: 0.9236 - val_precision_m: 0.9322 - val_f1_m: 0.9278\n",
      "Epoch 148/300\n",
      "122/155 [======================>.......] - ETA: 0s - loss: 0.1940 - accuracy: 0.9704 - recall_m: 0.9671 - precision_m: 0.9739 - f1_m: 0.9705\n",
      "Epoch 148: val_loss did not improve from 0.27442\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.1939 - accuracy: 0.9704 - recall_m: 0.9673 - precision_m: 0.9733 - f1_m: 0.9703 - val_loss: 0.3208 - val_accuracy: 0.9137 - val_recall_m: 0.9090 - val_precision_m: 0.9227 - val_f1_m: 0.9157\n",
      "Epoch 149/300\n",
      "131/155 [========================>.....] - ETA: 0s - loss: 0.1936 - accuracy: 0.9707 - recall_m: 0.9682 - precision_m: 0.9744 - f1_m: 0.9713\n",
      "Epoch 149: val_loss improved from 0.27442 to 0.27000, saving model to trained_models/fall_model_five_class.h5\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.1915 - accuracy: 0.9721 - recall_m: 0.9695 - precision_m: 0.9758 - f1_m: 0.9726 - val_loss: 0.2700 - val_accuracy: 0.9356 - val_recall_m: 0.9323 - val_precision_m: 0.9400 - val_f1_m: 0.9361\n",
      "Epoch 150/300\n",
      "138/155 [=========================>....] - ETA: 0s - loss: 0.1832 - accuracy: 0.9738 - recall_m: 0.9712 - precision_m: 0.9764 - f1_m: 0.9738\n",
      "Epoch 150: val_loss did not improve from 0.27000\n",
      "155/155 [==============================] - 0s 938us/step - loss: 0.1834 - accuracy: 0.9734 - recall_m: 0.9706 - precision_m: 0.9759 - f1_m: 0.9732 - val_loss: 0.3122 - val_accuracy: 0.9210 - val_recall_m: 0.9162 - val_precision_m: 0.9277 - val_f1_m: 0.9219\n",
      "Epoch 151/300\n",
      "137/155 [=========================>....] - ETA: 0s - loss: 0.1816 - accuracy: 0.9742 - recall_m: 0.9707 - precision_m: 0.9775 - f1_m: 0.9741\n",
      "Epoch 151: val_loss did not improve from 0.27000\n",
      "155/155 [==============================] - 0s 988us/step - loss: 0.1822 - accuracy: 0.9743 - recall_m: 0.9712 - precision_m: 0.9781 - f1_m: 0.9746 - val_loss: 0.3049 - val_accuracy: 0.9246 - val_recall_m: 0.9220 - val_precision_m: 0.9306 - val_f1_m: 0.9263\n",
      "Epoch 152/300\n",
      "139/155 [=========================>....] - ETA: 0s - loss: 0.1895 - accuracy: 0.9712 - recall_m: 0.9683 - precision_m: 0.9755 - f1_m: 0.9719\n",
      "Epoch 152: val_loss did not improve from 0.27000\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.1875 - accuracy: 0.9711 - recall_m: 0.9682 - precision_m: 0.9752 - f1_m: 0.9716 - val_loss: 0.3433 - val_accuracy: 0.9182 - val_recall_m: 0.9153 - val_precision_m: 0.9252 - val_f1_m: 0.9202\n",
      "Epoch 153/300\n",
      "130/155 [========================>.....] - ETA: 0s - loss: 0.1860 - accuracy: 0.9732 - recall_m: 0.9707 - precision_m: 0.9761 - f1_m: 0.9733\n",
      "Epoch 153: val_loss did not improve from 0.27000\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.1898 - accuracy: 0.9718 - recall_m: 0.9694 - precision_m: 0.9753 - f1_m: 0.9723 - val_loss: 0.3010 - val_accuracy: 0.9264 - val_recall_m: 0.9222 - val_precision_m: 0.9307 - val_f1_m: 0.9264\n",
      "Epoch 154/300\n",
      "146/155 [===========================>..] - ETA: 0s - loss: 0.1834 - accuracy: 0.9751 - recall_m: 0.9732 - precision_m: 0.9783 - f1_m: 0.9757\n",
      "Epoch 154: val_loss did not improve from 0.27000\n",
      "155/155 [==============================] - 0s 884us/step - loss: 0.1839 - accuracy: 0.9747 - recall_m: 0.9727 - precision_m: 0.9779 - f1_m: 0.9753 - val_loss: 0.2987 - val_accuracy: 0.9289 - val_recall_m: 0.9267 - val_precision_m: 0.9350 - val_f1_m: 0.9308\n",
      "Epoch 155/300\n",
      "153/155 [============================>.] - ETA: 0s - loss: 0.1888 - accuracy: 0.9737 - recall_m: 0.9711 - precision_m: 0.9770 - f1_m: 0.9740\n",
      "Epoch 155: val_loss did not improve from 0.27000\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.1887 - accuracy: 0.9736 - recall_m: 0.9712 - precision_m: 0.9771 - f1_m: 0.9741 - val_loss: 0.3164 - val_accuracy: 0.9286 - val_recall_m: 0.9246 - val_precision_m: 0.9326 - val_f1_m: 0.9286\n",
      "Epoch 156/300\n",
      "122/155 [======================>.......] - ETA: 0s - loss: 0.1838 - accuracy: 0.9734 - recall_m: 0.9708 - precision_m: 0.9769 - f1_m: 0.9738\n",
      "Epoch 156: val_loss did not improve from 0.27000\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.1853 - accuracy: 0.9728 - recall_m: 0.9707 - precision_m: 0.9766 - f1_m: 0.9736 - val_loss: 0.3265 - val_accuracy: 0.9225 - val_recall_m: 0.9198 - val_precision_m: 0.9280 - val_f1_m: 0.9239\n",
      "Epoch 157/300\n",
      "133/155 [========================>.....] - ETA: 0s - loss: 0.1853 - accuracy: 0.9739 - recall_m: 0.9696 - precision_m: 0.9768 - f1_m: 0.9732\n",
      "Epoch 157: val_loss did not improve from 0.27000\n",
      "155/155 [==============================] - 0s 932us/step - loss: 0.1881 - accuracy: 0.9733 - recall_m: 0.9694 - precision_m: 0.9765 - f1_m: 0.9729 - val_loss: 0.3223 - val_accuracy: 0.9301 - val_recall_m: 0.9260 - val_precision_m: 0.9334 - val_f1_m: 0.9297\n",
      "Epoch 158/300\n",
      "137/155 [=========================>....] - ETA: 0s - loss: 0.1904 - accuracy: 0.9733 - recall_m: 0.9707 - precision_m: 0.9754 - f1_m: 0.9730\n",
      "Epoch 158: val_loss did not improve from 0.27000\n",
      "155/155 [==============================] - 0s 929us/step - loss: 0.1908 - accuracy: 0.9727 - recall_m: 0.9699 - precision_m: 0.9749 - f1_m: 0.9724 - val_loss: 0.3187 - val_accuracy: 0.9280 - val_recall_m: 0.9237 - val_precision_m: 0.9322 - val_f1_m: 0.9279\n",
      "Epoch 159/300\n",
      "127/155 [=======================>......] - ETA: 0s - loss: 0.1831 - accuracy: 0.9755 - recall_m: 0.9724 - precision_m: 0.9774 - f1_m: 0.9749\n",
      "Epoch 159: val_loss did not improve from 0.27000\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.1840 - accuracy: 0.9747 - recall_m: 0.9717 - precision_m: 0.9769 - f1_m: 0.9743 - val_loss: 0.3190 - val_accuracy: 0.9286 - val_recall_m: 0.9254 - val_precision_m: 0.9341 - val_f1_m: 0.9297\n",
      "Epoch 160/300\n",
      "138/155 [=========================>....] - ETA: 0s - loss: 0.1964 - accuracy: 0.9693 - recall_m: 0.9665 - precision_m: 0.9722 - f1_m: 0.9693\n",
      "Epoch 160: val_loss did not improve from 0.27000\n",
      "155/155 [==============================] - 0s 936us/step - loss: 0.1942 - accuracy: 0.9701 - recall_m: 0.9674 - precision_m: 0.9729 - f1_m: 0.9702 - val_loss: 0.3016 - val_accuracy: 0.9350 - val_recall_m: 0.9326 - val_precision_m: 0.9369 - val_f1_m: 0.9347\n",
      "Epoch 161/300\n",
      "127/155 [=======================>......] - ETA: 0s - loss: 0.1929 - accuracy: 0.9705 - recall_m: 0.9675 - precision_m: 0.9729 - f1_m: 0.9702\n",
      "Epoch 161: val_loss did not improve from 0.27000\n",
      "155/155 [==============================] - 0s 2ms/step - loss: 0.1923 - accuracy: 0.9700 - recall_m: 0.9671 - precision_m: 0.9728 - f1_m: 0.9700 - val_loss: 0.3338 - val_accuracy: 0.9146 - val_recall_m: 0.9106 - val_precision_m: 0.9188 - val_f1_m: 0.9147\n",
      "Epoch 162/300\n",
      "140/155 [==========================>...] - ETA: 0s - loss: 0.1853 - accuracy: 0.9750 - recall_m: 0.9719 - precision_m: 0.9781 - f1_m: 0.9750\n",
      "Epoch 162: val_loss did not improve from 0.27000\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.1843 - accuracy: 0.9755 - recall_m: 0.9723 - precision_m: 0.9781 - f1_m: 0.9752 - val_loss: 0.3144 - val_accuracy: 0.9277 - val_recall_m: 0.9257 - val_precision_m: 0.9305 - val_f1_m: 0.9281\n",
      "Epoch 163/300\n",
      "124/155 [=======================>......] - ETA: 0s - loss: 0.1891 - accuracy: 0.9720 - recall_m: 0.9689 - precision_m: 0.9753 - f1_m: 0.9720\n",
      "Epoch 163: val_loss did not improve from 0.27000\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.1862 - accuracy: 0.9732 - recall_m: 0.9701 - precision_m: 0.9758 - f1_m: 0.9729 - val_loss: 0.3223 - val_accuracy: 0.9277 - val_recall_m: 0.9265 - val_precision_m: 0.9299 - val_f1_m: 0.9282\n",
      "Epoch 164/300\n",
      "144/155 [==========================>...] - ETA: 0s - loss: 0.1898 - accuracy: 0.9711 - recall_m: 0.9688 - precision_m: 0.9739 - f1_m: 0.9713\n",
      "Epoch 164: val_loss did not improve from 0.27000\n",
      "155/155 [==============================] - 0s 902us/step - loss: 0.1905 - accuracy: 0.9704 - recall_m: 0.9681 - precision_m: 0.9734 - f1_m: 0.9708 - val_loss: 0.3267 - val_accuracy: 0.9237 - val_recall_m: 0.9203 - val_precision_m: 0.9268 - val_f1_m: 0.9235\n",
      "Epoch 165/300\n",
      "123/155 [======================>.......] - ETA: 0s - loss: 0.1868 - accuracy: 0.9737 - recall_m: 0.9710 - precision_m: 0.9768 - f1_m: 0.9739\n",
      "Epoch 165: val_loss did not improve from 0.27000\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.1882 - accuracy: 0.9727 - recall_m: 0.9702 - precision_m: 0.9761 - f1_m: 0.9731 - val_loss: 0.3099 - val_accuracy: 0.9219 - val_recall_m: 0.9196 - val_precision_m: 0.9255 - val_f1_m: 0.9225\n",
      "Epoch 166/300\n",
      "149/155 [===========================>..] - ETA: 0s - loss: 0.1921 - accuracy: 0.9725 - recall_m: 0.9699 - precision_m: 0.9756 - f1_m: 0.9727\n",
      "Epoch 166: val_loss did not improve from 0.27000\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.1919 - accuracy: 0.9725 - recall_m: 0.9690 - precision_m: 0.9753 - f1_m: 0.9721 - val_loss: 0.3181 - val_accuracy: 0.9243 - val_recall_m: 0.9216 - val_precision_m: 0.9295 - val_f1_m: 0.9255\n",
      "Epoch 167/300\n",
      "144/155 [==========================>...] - ETA: 0s - loss: 0.1856 - accuracy: 0.9737 - recall_m: 0.9698 - precision_m: 0.9774 - f1_m: 0.9736\n",
      "Epoch 167: val_loss did not improve from 0.27000\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.1843 - accuracy: 0.9741 - recall_m: 0.9700 - precision_m: 0.9774 - f1_m: 0.9737 - val_loss: 0.3398 - val_accuracy: 0.9289 - val_recall_m: 0.9231 - val_precision_m: 0.9340 - val_f1_m: 0.9285\n",
      "Epoch 168/300\n",
      "144/155 [==========================>...] - ETA: 0s - loss: 0.1949 - accuracy: 0.9710 - recall_m: 0.9678 - precision_m: 0.9745 - f1_m: 0.9711\n",
      "Epoch 168: val_loss did not improve from 0.27000\n",
      "155/155 [==============================] - 0s 909us/step - loss: 0.1937 - accuracy: 0.9713 - recall_m: 0.9680 - precision_m: 0.9748 - f1_m: 0.9713 - val_loss: 0.2977 - val_accuracy: 0.9340 - val_recall_m: 0.9321 - val_precision_m: 0.9376 - val_f1_m: 0.9348\n",
      "Epoch 169/300\n",
      "131/155 [========================>.....] - ETA: 0s - loss: 0.1867 - accuracy: 0.9727 - recall_m: 0.9697 - precision_m: 0.9758 - f1_m: 0.9727\n",
      "Epoch 169: val_loss did not improve from 0.27000\n",
      "155/155 [==============================] - 0s 1ms/step - loss: 0.1867 - accuracy: 0.9728 - recall_m: 0.9690 - precision_m: 0.9754 - f1_m: 0.9722 - val_loss: 0.3492 - val_accuracy: 0.9219 - val_recall_m: 0.9195 - val_precision_m: 0.9255 - val_f1_m: 0.9225\n",
      "Epoch 169: early stopping\n",
      "\n",
      "103/103 [==============================] - 0s 532us/step\n",
      "103/103 [==============================] - 0s 404us/step - loss: 0.2722 - accuracy: 0.9438 - recall_m: 0.9419 - precision_m: 0.9458 - f1_m: 0.9438\n",
      "test result correct percent : 94.38%\n"
     ]
    }
   ],
   "source": [
    "#   (multi-class classification)\n",
    "#  \n",
    "learn_csv_file = pd.read_csv('csv_files/side_five_class/side_learn.csv')\n",
    "valid_csv_file = pd.read_csv('csv_files/side_five_class/side_valid.csv')\n",
    "test_csv_file = pd.read_csv('csv_files/side_five_class/side_test.csv')\n",
    "\n",
    "learn_csv_file = sklearn.utils.shuffle(learn_csv_file)\n",
    "valid_csv_file = sklearn.utils.shuffle(valid_csv_file)\n",
    "\n",
    "col = 34\n",
    "\n",
    "learn_x = learn_csv_file.iloc[:,:col].values\n",
    "learn_y = learn_csv_file.iloc[:,col:].values\n",
    "valid_x = valid_csv_file.iloc[:,:col].values\n",
    "valid_y = valid_csv_file.iloc[:,col:].values\n",
    "test_x = test_csv_file.iloc[:,:col].values\n",
    "test_y = test_csv_file.iloc[:,col:].values\n",
    "\n",
    "scaler = RobustScaler()\n",
    "scaler.fit(learn_x)\n",
    "scaled_learn_x = scaler.transform(learn_x)\n",
    "scaled_valid_x = scaler.transform(valid_x)\n",
    "scaled_test_x = scaler.transform(test_x)\n",
    "joblib.dump(scaler, 'trained_models/scalers/fall_scaler_robust_five_class.pkl')\n",
    "\n",
    "# one-hot encoding\n",
    "learn_y = tf.keras.utils.to_categorical(learn_y)\n",
    "valid_y = tf.keras.utils.to_categorical(valid_y)\n",
    "test_y = tf.keras.utils.to_categorical(test_y)\n",
    "\n",
    "dropout_ratio = 0.5\n",
    "regulation = 0.01\n",
    "\n",
    "side_model = Sequential()\n",
    "\n",
    "side_model.add(Dense(128, activation='relu', input_shape=(col,), kernel_regularizer=regularizers.l2(regulation)))\n",
    "side_model.add(Dropout(dropout_ratio))\n",
    "side_model.add(Dense(128, activation='relu', input_shape=(col,), kernel_regularizer=regularizers.l2(regulation)))\n",
    "side_model.add(Dropout(dropout_ratio))\n",
    "side_model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "#       \n",
    "# adam, crossentropy\n",
    "side_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy', recall_m, precision_m, f1_m])\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=20, verbose=1)\n",
    "save_best = keras.callbacks.ModelCheckpoint(filepath='trained_models/fall_model_five_class.h5', verbose=1, save_best_only=True, mode='auto', save_freq='epoch')\n",
    "\n",
    "#  training\n",
    "side_model_after_data = side_model.fit(scaled_learn_x, learn_y, batch_size=64, epochs=300, validation_data=(scaled_valid_x, valid_y), callbacks=[early_stop, save_best])\n",
    "\n",
    "#  test (predict)\n",
    "print()\n",
    "predict_result = side_model.predict(scaled_test_x)\n",
    "side_model.evaluate(scaled_test_x, test_y)\n",
    "test_correct = 0\n",
    "for i in range(len(test_y)):\n",
    "    if test_y[i].argmax() == predict_result[i].argmax():\n",
    "        test_correct += 1\n",
    "print(f\"test result correct percent : {test_correct/len(test_y) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1080x216 with 3 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# \n",
    "# ,  \n",
    "plt.subplots(figsize=(15, 3), constrained_layout=True)\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.plot(side_model_after_data.history['loss'])\n",
    "plt.plot(side_model_after_data.history['val_loss'])\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.plot(side_model_after_data.history['accuracy'])\n",
    "plt.plot(side_model_after_data.history['val_accuracy'])\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('f1 score')\n",
    "plt.plot(side_model_after_data.history['f1_m'])\n",
    "plt.plot(side_model_after_data.history['val_f1_m'])\n",
    "\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}